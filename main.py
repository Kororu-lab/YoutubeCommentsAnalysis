"""
YouTube Comments Analysis - Main Execution Script
ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ - í†µí•© ë¶„ì„ ëª¨ë“œ
"""

import os
import sys
import logging
import traceback
import argparse
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import AnalysisConfig, validate_config
from src.data_processor import DataProcessor
from src.data_filter import DataFilter
from src.adaptive_time_analyzer import AdaptiveTimeAnalyzer
from src.sentiment_analyzer import SentimentAnalyzer
from src.topic_analyzer import TopicAnalyzer
from src.visualizer import Visualizer
from src.report_generator import ReportGenerator

# ê³ ê¸‰ ë¶„ì„ ëª¨ë“ˆë“¤ (ì„ íƒì  import)
try:
    from src.advanced_frame_analyzer import AdvancedFrameAnalyzer
    from src.advanced_visualizer import AdvancedVisualizer
    ADVANCED_MODULES_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ ê³ ê¸‰ ë¶„ì„ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    ADVANCED_MODULES_AVAILABLE = False

def setup_logging(config):
    """ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(
        level=getattr(logging, config.LOGGING['level']),
        format=config.LOGGING['format'],
        handlers=[
            logging.FileHandler(config.LOGGING['file'], encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

def run_basic_analysis(config: AnalysisConfig, logger: logging.Logger):
    """ê¸°ë³¸ ë¶„ì„ ì‹¤í–‰"""
    try:
        target_name = config.TARGET_NAME
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        for dir_path in config.OUTPUT_STRUCTURE.values():
            os.makedirs(dir_path, exist_ok=True)
        
        # 1. ë°ì´í„° ì²˜ë¦¬
        logger.info(f"ğŸ“Š {target_name} ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
        data_processor = DataProcessor(config)
        
        # CSV íŒŒì¼ ë¡œë“œ
        csv_path = config.DATA_FILES['youtube_comments']
        data_processor.load_data(csv_path)
        
        # ì „ì²´ ë°ì´í„°ë¥¼ ë‹¨ì¼ íƒ€ê²Ÿìœ¼ë¡œ ì²˜ë¦¬ (í‚¤ì›Œë“œ í•„í„°ë§ ì—†ì´)
        df = data_processor.raw_data
        if df is None or len(df) == 0:
            logger.error(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ëŒ“ê¸€ ì—†ìŒ")
            return None
        
        # ëŒ“ê¸€ ì „ì²˜ë¦¬
        processed_df = data_processor.preprocess_comments(df, target_name)
        
        # ë°ì´í„° í•„í„°ë§ ì ìš©
        logger.info(f"ğŸ” {target_name} ë°ì´í„° í•„í„°ë§ ì‹œì‘")
        data_filter = DataFilter(config)
        
        # í•„í„°ë§ ì„¤ì • ê²€ì¦
        if not data_filter.validate_filtering_config():
            logger.warning("âš ï¸ í•„í„°ë§ ì„¤ì •ì— ë¬¸ì œê°€ ìˆì–´ í•„í„°ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            filtered_df = processed_df
        else:
            # ëª¨ë“  í•„í„°ë§ ì ìš©
            filtered_df = data_filter.apply_all_filters(processed_df)
            
            # í•„í„°ë§ í†µê³„ ë¡œê¹…
            filter_stats = data_filter.get_filtering_stats()
            logger.info(f"ğŸ“Š í•„í„°ë§ ì™„ë£Œ: {filter_stats['original_count']:,}ê°œ â†’ {filter_stats['final_count']:,}ê°œ ëŒ“ê¸€")
        
        # ì ì‘ì  ì‹œê°„ ë¶„ì„ (config.py ì„¤ì • ê¸°ë°˜)
        logger.info(f"ğŸ“Š {target_name} ì ì‘ì  ì‹œê°„ ë¶„ì„ ì‹œì‘")
        time_analyzer = AdaptiveTimeAnalyzer(config)
        time_distribution = None  # ì´ˆê¸°í™”
        
        # ì ì‘ì  ì„¸ë¶„í™” ê·¸ë£¹ ìƒì„± (config.pyì˜ ADAPTIVE_TIME_ANALYSIS ì„¤ì • ì‚¬ìš©)
        if config.ADAPTIVE_TIME_ANALYSIS['enabled']:
            logger.info(f"ğŸ”„ ì ì‘ì  ì‹œê°„ ì„¸ë¶„í™” í™œì„±í™” (ì„ê³„ê°’: {config.ADAPTIVE_TIME_ANALYSIS['high_ratio_threshold']})")
            time_groups_result = time_analyzer.create_adaptive_subdivided_groups(filtered_df)
            time_groups = time_groups_result['groups']
            optimal_unit = 'adaptive_subdivided'
            
            # ì„¸ë¶„í™” ì •ë³´ ë¡œê¹…
            metadata = time_groups_result.get('metadata', {})
            logger.info(f"ğŸ“… ì ì‘ì  ì„¸ë¶„í™” ì™„ë£Œ: {metadata.get('total_periods', 0)}ê°œ êµ¬ê°„")
            logger.info(f"ğŸ”§ ì„¸ë¶„í™” ì ìš©: {metadata.get('subdivisions_applied', 0)}ê°œ ê¸°ê°„")
            
            # ì ì‘ì  ì„¸ë¶„í™”ìš© ê°€ìƒ time_distribution ìƒì„±
            time_distribution = {
                'optimal_time_unit': optimal_unit,
                'total_comments': len(filtered_df),
                'total_periods': metadata.get('total_periods', 0),
                'subdivisions_applied': metadata.get('subdivisions_applied', 0),
                'recommendation': {
                    'time_unit': optimal_unit,
                    'reasons': ['ì ì‘ì  ì‹œê°„ ì„¸ë¶„í™”ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.'],
                    'warnings': [],
                    'suggestions': []
                }
            }
        else:
            # ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            time_distribution = time_analyzer.analyze_time_distribution(filtered_df)
            
            if time_distribution:
                optimal_unit = time_distribution['optimal_time_unit']
                logger.info(f"ğŸ“… ìµœì  ì‹œê°„ ë‹¨ìœ„: {optimal_unit}")
                
                # ì ì‘ì  ì‹œê°„ ê·¸ë£¹ ìƒì„±
                time_groups_result = time_analyzer.create_adaptive_time_groups(filtered_df, optimal_unit)
                time_groups = time_groups_result['groups']
                
                # ê¶Œì¥ì‚¬í•­ ë¡œê¹…
                recommendation = time_distribution.get('recommendation', {})
                for reason in recommendation.get('reasons', []):
                    logger.info(f"ğŸ’¡ {reason}")
                for warning in recommendation.get('warnings', []):
                    logger.warning(f"âš ï¸ {warning}")
                for suggestion in recommendation.get('suggestions', []):
                    logger.info(f"ğŸ”§ {suggestion}")
            else:
                logger.warning("âš ï¸ ì‹œê°„ ë¶„í¬ ë¶„ì„ ì‹¤íŒ¨, ì›”ë³„ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                time_groups = data_processor.group_by_month_single(filtered_df)
                optimal_unit = 'monthly_fallback'
                # í´ë°±ìš© time_distribution ìƒì„±
                time_distribution = {
                    'optimal_time_unit': optimal_unit,
                    'total_comments': len(filtered_df),
                    'recommendation': {
                        'time_unit': optimal_unit,
                        'reasons': ['ì‹œê°„ ë¶„í¬ ë¶„ì„ ì‹¤íŒ¨ë¡œ ì›”ë³„ ë¶„ì„ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.'],
                        'warnings': ['ë°ì´í„° í’ˆì§ˆì´ ì´ìƒì ì´ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.'],
                        'suggestions': []
                    }
                }
        
        # ì‹œê°„ ë¶„í¬ ì‹œê°í™”
        time_dist_plot = time_analyzer.visualize_time_distribution(
            {'optimal_time_unit': optimal_unit}, 
            df=filtered_df, 
            date_column='date'
        )
        logger.info(f"ğŸ“Š ì‹œê°„ ë¶„í¬ ë¶„ì„ ì™„ë£Œ: {time_dist_plot}")
        
        logger.info(f"âœ… ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {len(filtered_df):,}ê°œ ëŒ“ê¸€ (í•„í„°ë§ í›„)")
        logger.info(f"ğŸ“… ë¶„ì„ ê¸°ê°„: {len(time_groups)}ê°œ {optimal_unit} ê·¸ë£¹")
        
        # 2. ê°ì„± ë¶„ì„ (ì ì‘ì  ì‹œê°„ ê·¸ë£¹ ì‚¬ìš©)
        logger.info(f"ğŸ˜Š {target_name} ê°ì„± ë¶„ì„ ì‹œì‘")
        sentiment_analyzer = SentimentAnalyzer(config)
        sentiment_results = sentiment_analyzer.analyze_time_grouped_sentiment(time_groups, target_name)
        sentiment_trends = sentiment_analyzer.get_sentiment_trends(target_name)
        
        # 3. í† í”½ ë¶„ì„ (ì ì‘ì  ì‹œê°„ ê·¸ë£¹ ì‚¬ìš©) - ì¡°ê±´ë¶€ ì‹¤í–‰
        topic_analyzer = TopicAnalyzer(config)
        if config.TOPIC_ANALYSIS_ENABLED:
            logger.info(f"ğŸ” {target_name} í† í”½ ë¶„ì„ ì‹œì‘")
            topic_results = topic_analyzer.analyze_time_grouped_topics(time_groups, target_name)
            topic_evolution = topic_analyzer.get_topic_evolution(target_name)
        else:
            logger.info(f"âš ï¸ í† í”½ ë¶„ì„ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ ì¶”ì¶œë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            topic_results = {}
            topic_evolution = {}
        
        # 4. ì‹œê°í™” ìƒì„± (ê°œì„ ëœ ë²„ì „)
        logger.info(f"ğŸ“Š {target_name} ì‹œê°í™” ìƒì„± ì‹œì‘")
        visualizer = Visualizer(config)
        
        # 5. ì¢…í•© ê²°ê³¼ êµ¬ì„± ë¨¼ì € ìˆ˜í–‰
        comprehensive_results = {}
        
        for group_name in time_groups.keys():
            # ê¸°ë³¸ ì¢…í•© ê²°ê³¼
            comprehensive_results[group_name] = {
                'total_comments': len(time_groups[group_name]),
                'binary_sentiment': sentiment_results[group_name]['binary_sentiment'] if group_name in sentiment_results else {},
                'emotion_sentiment': sentiment_results[group_name]['emotion_sentiment'] if group_name in sentiment_results else {},
                'bertopic': topic_results[group_name]['bertopic'] if group_name in topic_results else {},
                'lda': topic_results[group_name]['lda'] if group_name in topic_results else {}
            }
        
        # ì‹œê°„ ê·¸ë£¹ë³„ ë¹„êµ ì‹œê°í™” (ê°ì„± ì¤‘ì‹¬)
        time_comparison_plot = visualizer.plot_time_grouped_comparison(sentiment_results, target_name, optimal_unit)
        
        # í† í”½ ë¶„ì„ ëŒ€ì‹œë³´ë“œ (ìƒˆë¡œìš´ í† í”½ ì¤‘ì‹¬ ì‹œê°í™”) - ì¡°ê±´ë¶€ ì‹¤í–‰
        if config.TOPIC_ANALYSIS_ENABLED:
            topic_dashboard_plot = visualizer.create_topic_analysis_dashboard(
                comprehensive_results, target_name, optimal_unit, min_comments_threshold=50
            )
        else:
            topic_dashboard_plot = None
        
        # ê¸°ê°„ë³„ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± (ëª¨ë“  ì‹œê°„ ê·¸ë£¹ì— ëŒ€í•´)
        logger.info(f"â˜ï¸ {target_name} ê¸°ê°„ë³„ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹œì‘")
        time_group_wordclouds = []
        
        for group_name, group_data in time_groups.items():
            if len(group_data) >= 10:  # ìµœì†Œ 10ê°œ ëŒ“ê¸€ì´ ìˆëŠ” ê·¸ë£¹ë§Œ
                try:
                    # í•´ë‹¹ ê·¸ë£¹ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    group_texts = group_data['cleaned_text'].tolist()
                    
                    # í‚¤ì›Œë“œ ì¶”ì¶œ
                    group_keywords = topic_analyzer.extract_keywords(group_texts, top_k=50)
                    
                    if group_keywords:
                        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
                        wordcloud_path = visualizer.create_wordcloud(group_keywords, target_name, group_name)
                        if wordcloud_path:
                            time_group_wordclouds.append(wordcloud_path)
                            logger.info(f"â˜ï¸ {group_name} ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì™„ë£Œ: {len(group_keywords)}ê°œ í‚¤ì›Œë“œ")
                    else:
                        logger.warning(f"âš ï¸ {group_name}: í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨")
                        
                except Exception as e:
                    logger.error(f"âŒ {group_name} ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            else:
                logger.info(f"âš ï¸ {group_name}: ëŒ“ê¸€ ìˆ˜ ë¶€ì¡±({len(group_data)}ê°œ), ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ê±´ë„ˆëœ€")
        
        logger.info(f"â˜ï¸ ê¸°ê°„ë³„ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì™„ë£Œ: {len(time_group_wordclouds)}ê°œ")
        
        # ê°ì„± íŠ¸ë Œë“œ ì‹œê°í™” (6ê°ì • í¬í•¨)
        sentiment_plot = visualizer.plot_sentiment_trends(sentiment_trends, target_name)
        
        # ì „ì²´ ê¸°ê°„ ì›Œë“œí´ë¼ìš°ë“œ (ì°¸ê³ ìš©)
        all_texts = []
        for group_data in time_groups.values():
            all_texts.extend(group_data['cleaned_text'].tolist())
        
        keywords = topic_analyzer.extract_keywords(all_texts, top_k=50)
        overall_wordcloud_path = visualizer.create_wordcloud(keywords, target_name, "ì „ì²´ê¸°ê°„")
        
        # 6. ìƒì„¸ ë°ì´í„° ì €ì¥ êµ¬ì„±
        time_group_detailed_results = {}
        
        for group_name in time_groups.keys():
            # ìƒì„¸ ì‹œê°„ ê·¸ë£¹ë³„ ê²°ê³¼ (CSV ì €ì¥ìš©)
            group_texts = time_groups[group_name]['cleaned_text'].tolist()
            group_keywords = topic_analyzer.extract_keywords(group_texts, top_k=20)
            
            time_group_detailed_results[group_name] = {
                'time_group': group_name,
                'total_comments': len(time_groups[group_name]),
                'avg_comment_length': time_groups[group_name]['cleaned_text'].str.len().mean(),
                'top_keywords': [kw[0] for kw in group_keywords[:10]] if group_keywords else [],
                'keyword_scores': [kw[1] for kw in group_keywords[:10]] if group_keywords else [],
                'sentiment_summary': sentiment_results[group_name]['binary_sentiment'] if group_name in sentiment_results else {},
                'emotion_summary': sentiment_results[group_name]['emotion_sentiment'] if group_name in sentiment_results else {},
                'topic_summary': {
                    'bertopic_topics': len(topic_results[group_name]['bertopic'].get('topics', [])) if group_name in topic_results else 0,
                    'lda_topics': len(topic_results[group_name]['lda'].get('topics', [])) if group_name in topic_results else 0
                }
            }
        
        # ì‹œê°„ ê·¸ë£¹ë³„ ìƒì„¸ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
        import pandas as pd
        time_group_summary_data = []
        for group_name, data in time_group_detailed_results.items():
            row = {
                'time_group': group_name,
                'time_unit': optimal_unit,
                'total_comments': data['total_comments'],
                'avg_comment_length': round(data['avg_comment_length'], 2),
                'top_keywords': ', '.join(data['top_keywords'][:5]),
                'positive_ratio': data['sentiment_summary'].get('positive_ratio', 0),
                'negative_ratio': data['sentiment_summary'].get('negative_ratio', 0),
                'dominant_emotion': data['emotion_summary'].get('dominant_emotion', 'ì—†ìŒ'),
                'emotion_confidence': data['emotion_summary'].get('confidence', 0),
                'bertopic_topics': data['topic_summary']['bertopic_topics'],
                'lda_topics': data['topic_summary']['lda_topics']
            }
            time_group_summary_data.append(row)
        
        time_group_summary_df = pd.DataFrame(time_group_summary_data)
        time_group_summary_path = os.path.join(config.OUTPUT_STRUCTURE['data_processed'], f'{target_name}_{optimal_unit}_summary.csv')
        time_group_summary_df.to_csv(time_group_summary_path, index=False, encoding='utf-8-sig')
        logger.info(f"ğŸ“Š ì‹œê°„ ê·¸ë£¹ë³„ ìš”ì•½ ë°ì´í„° ì €ì¥: {time_group_summary_path}")
        
        # í‚¤ì›Œë“œ ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì €ì¥
        keyword_detailed_data = []
        for group_name, data in time_group_detailed_results.items():
            for i, (keyword, score) in enumerate(zip(data['top_keywords'], data['keyword_scores'])):
                keyword_detailed_data.append({
                    'time_group': group_name,
                    'time_unit': optimal_unit,
                    'rank': i + 1,
                    'keyword': keyword,
                    'score': score,
                    'total_comments': data['total_comments']
                })
        
        if keyword_detailed_data:
            keyword_detailed_df = pd.DataFrame(keyword_detailed_data)
            keyword_detailed_path = os.path.join(config.OUTPUT_STRUCTURE['data_processed'], f'{target_name}_keywords_detailed.csv')
            keyword_detailed_df.to_csv(keyword_detailed_path, index=False, encoding='utf-8-sig')
            logger.info(f"ğŸ” í‚¤ì›Œë“œ ìƒì„¸ ë°ì´í„° ì €ì¥: {keyword_detailed_path}")
        
        # 6. ë³´ê³ ì„œ ìƒì„±
        logger.info(f"ğŸ“‹ {target_name} ë³´ê³ ì„œ ìƒì„± ì‹œì‘")
        report_generator = ReportGenerator(config)
        report_path = report_generator.generate_comprehensive_report(comprehensive_results, target_name)
        
        # 7. ì¸í„°ë™í‹°ë¸Œ ëŒ€ì‹œë³´ë“œ ìƒì„±
        dashboard_path = visualizer.create_interactive_dashboard(comprehensive_results, target_name)
        
        logger.info(f"âœ… {target_name} ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ")
        
        # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
        processed_result = {
            'total_comments': len(filtered_df),
            'data': filtered_df,
            'filtering_stats': data_filter.get_filtering_stats() if 'data_filter' in locals() else None,
            'time_groups': time_groups,
            'time_analysis': {
                'optimal_unit': optimal_unit,
                'distribution_result': time_distribution,
                'visualization_path': time_dist_plot
            },
            'date_range': {
                'start': filtered_df['date'].min() if 'date' in filtered_df.columns else None,
                'end': filtered_df['date'].max() if 'date' in filtered_df.columns else None
            }
        }
        
        return {
            'target_name': target_name,
            'processed_data': processed_result,
            'sentiment_results': sentiment_results,
            'sentiment_trends': sentiment_trends,
            'topic_results': topic_results,
            'topic_evolution': topic_evolution,
            'comprehensive_results': comprehensive_results,
            'visualizations': {
                'sentiment_plot': sentiment_plot,
                'topic_dashboard_plot': topic_dashboard_plot,
                'time_comparison_plot': time_comparison_plot,
                'time_distribution_plot': time_dist_plot,
                'overall_wordcloud': overall_wordcloud_path,
                'time_group_wordclouds': time_group_wordclouds,
                'dashboard': dashboard_path
            },
            'reports': {
                'comprehensive_report': report_path
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ {target_name} ê¸°ë³¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def run_advanced_analysis(config: AnalysisConfig, logger: logging.Logger, basic_result: dict):
    """ê³ ê¸‰ ë¶„ì„ ì‹¤í–‰"""
    try:
        if not ADVANCED_MODULES_AVAILABLE:
            logger.error("âŒ ê³ ê¸‰ ë¶„ì„ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        target_name = config.TARGET_NAME
        logger.info(f"ğŸš€ {target_name} ê³ ê¸‰ ë¶„ì„ ì‹œì‘")
        
        # ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        filtered_df = basic_result['processed_data']['data']
        time_groups = basic_result['processed_data']['time_groups']
        
        # ê³ ê¸‰ ë¶„ì„ê¸° ì´ˆê¸°í™”
        advanced_analyzer = AdvancedFrameAnalyzer(config)
        advanced_visualizer = AdvancedVisualizer(config)
        
        # 1. ì‹œê°„ ê¸°ë°˜ ì—¬ë¡  íë¦„ ë¶„ì„
        logger.info(f"ğŸ“Š {target_name} ì‹œê°„ ê¸°ë°˜ ì—¬ë¡  íë¦„ ë¶„ì„")
        temporal_flow = advanced_analyzer.analyze_temporal_opinion_flow(filtered_df, target_name)
        
        # 2. ì¢…í•©ì  í† í”½ ë¶„ì„
        logger.info(f"ğŸ” {target_name} ì¢…í•© í† í”½ ë¶„ì„")
        comprehensive_topics = advanced_analyzer.analyze_topics_comprehensive(
            filtered_df, target_name, methods=['lda', 'nmf', 'bertopic']
        )
        
        # 3. í‚¤ì›Œë“œ ê³µì¶œí˜„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„
        logger.info(f"ğŸ•¸ï¸ {target_name} í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„")
        keyword_network = advanced_analyzer.analyze_keyword_cooccurrence_network(
            filtered_df, target_name, min_cooccurrence=3
        )
        
        # 4. ë³€ê³¡ì  ë° ì´ìƒì¹˜ íƒì§€
        logger.info(f"ğŸ“ˆ {target_name} ë³€ê³¡ì  íƒì§€")
        anomalies = advanced_analyzer.detect_anomalies_and_changepoints(filtered_df, target_name)
        
        # 5. ë¬¸ë§¥ ì„ë² ë”© í´ëŸ¬ìŠ¤í„°ë§
        logger.info(f"ğŸ§  {target_name} ë¬¸ë§¥ ì„ë² ë”© ë¶„ì„")
        contextual_clusters = advanced_analyzer.analyze_contextual_embeddings(filtered_df, target_name)
        
        # 6. ê³ ê¸‰ ì‹œê°í™” ìƒì„±
        logger.info(f"ğŸ“Š {target_name} ê³ ê¸‰ ì‹œê°í™” ìƒì„±")
        
        # ì‹œê°„ë³„ ì—¬ë¡  íë¦„ ì‹œê°í™”
        temporal_flow_plot = advanced_visualizer.visualize_temporal_opinion_flow(
            temporal_flow, target_name
        )
        
        # í† í”½ ì§„í™” ì‹œê°í™”
        topic_evolution_plot = advanced_visualizer.visualize_topic_evolution(
            comprehensive_topics, target_name
        )
        
        # í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
        network_plot = advanced_visualizer.visualize_keyword_network(
            keyword_network, target_name
        )
        
        # ì¢…í•© ëŒ€ì‹œë³´ë“œ ìƒì„±
        comprehensive_dashboard = advanced_visualizer.create_comprehensive_dashboard(
            {
                'temporal_flow': temporal_flow,
                'topics': comprehensive_topics,
                'network': keyword_network,
                'anomalies': anomalies,
                'clusters': contextual_clusters
            },
            target_name
        )
        
        # 7. ê³ ê¸‰ ë¶„ì„ ê²°ê³¼ ì €ì¥
        advanced_results = {
            'temporal_flow': temporal_flow,
            'comprehensive_topics': comprehensive_topics,
            'keyword_network': keyword_network,
            'anomalies': anomalies,
            'contextual_clusters': contextual_clusters
        }
        
        # ê²°ê³¼ ì €ì¥
        advanced_results_path = os.path.join(
            config.OUTPUT_STRUCTURE['data_processed'], 
            f'{target_name}_advanced_analysis.pkl'
        )
        advanced_analyzer.save_analysis_results(advanced_results_path)
        
        logger.info(f"âœ… {target_name} ê³ ê¸‰ ë¶„ì„ ì™„ë£Œ")
        
        return {
            'advanced_results': advanced_results,
            'advanced_visualizations': {
                'temporal_flow_plot': temporal_flow_plot,
                'topic_evolution_plot': topic_evolution_plot,
                'network_plot': network_plot,
                'comprehensive_dashboard': comprehensive_dashboard
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ {target_name} ê³ ê¸‰ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì¸ì íŒŒì„œ ì„¤ì •
    parser = argparse.ArgumentParser(description='YouTube ëŒ“ê¸€ ë¶„ì„ ë„êµ¬')
    parser.add_argument('--basic-only', action='store_true', 
                       help='ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰ (ê°ì„± ë¶„ì„ + í‚¤ì›Œë“œ ì¶”ì¶œ)')
    
    args = parser.parse_args()
    
    # ë¶„ì„ ìˆ˜í–‰
    if args.basic_only:
        # ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰
        print("ğŸ“Š ê¸°ë³¸ ë¶„ì„ ëª¨ë“œ")
        try:
            # 1. ì„¤ì • ê²€ì¦
            print("ğŸ”§ ì„¤ì • ê²€ì¦ ì¤‘...")
            if not validate_config():
                print("âŒ ì„¤ì • ê²€ì¦ ì‹¤íŒ¨")
                return
            
            config = AnalysisConfig()
            logger = setup_logging(config)
            
            logger.info("ğŸš€ YouTube Comments Analysis ì‹œì‘")
            logger.info(f"ğŸ“± ë””ë°”ì´ìŠ¤: {config.DEVICE}")
            logger.info(f"ğŸ¯ ë¶„ì„ ëª¨ë“œ: {config.ANALYSIS_MODE}")
            logger.info(f"ğŸ“„ ë¶„ì„ íŒŒì¼: {config.DATA_FILES['youtube_comments']}")
            
            # 2. ê¸°ë³¸ ë¶„ì„ ì‹¤í–‰
            print(f"\nğŸ¯ {config.TARGET_NAME} ê¸°ë³¸ ë¶„ì„ ì‹œì‘...")
            
            basic_result = run_basic_analysis(config, logger)
            
            if not basic_result:
                print("âŒ ê¸°ë³¸ ë¶„ì„ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return
            
            print(f"âœ… {config.TARGET_NAME} ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ")
            
            # ì£¼ìš” ê²°ê³¼ ì¶œë ¥
            total_comments = basic_result['processed_data']['total_comments']
            date_range = basic_result['processed_data']['date_range']
            
            print(f"   ğŸ“Š ì´ ëŒ“ê¸€ ìˆ˜: {total_comments:,}ê°œ")
            if date_range['start'] and date_range['end']:
                print(f"   ğŸ“… ë¶„ì„ ê¸°ê°„: {date_range['start'].strftime('%Y-%m-%d')} ~ {date_range['end'].strftime('%Y-%m-%d')}")
            
            # ê°ì„± ë¶„ì„ ìš”ì•½
            if basic_result['sentiment_trends']:
                months = basic_result['sentiment_trends']['months']
                if months:
                    avg_positive = sum(basic_result['sentiment_trends']['positive_ratios']) / len(months)
                    avg_negative = sum(basic_result['sentiment_trends']['negative_ratios']) / len(months)
                    print(f"   ğŸ˜Š í‰ê·  ê¸ì • ë¹„ìœ¨: {avg_positive:.1%}")
                    print(f"   ğŸ˜ í‰ê·  ë¶€ì • ë¹„ìœ¨: {avg_negative:.1%}")
            
            # í† í”½ ë¶„ì„ ìš”ì•½
            if basic_result['topic_results']:
                bertopic_months = len([m for m, r in basic_result['topic_results'].items() 
                                     if r.get('bertopic') and r['bertopic'].get('topic_labels')])
                lda_months = len([m for m, r in basic_result['topic_results'].items() 
                                if r.get('lda') and r['lda'].get('topic_labels')])
                print(f"   ğŸ” BERTopic ë¶„ì„ ì™„ë£Œ: {bertopic_months}ê°œì›”")
                print(f"   ğŸ“š LDA ë¶„ì„ ì™„ë£Œ: {lda_months}ê°œì›”")
            
            # 4. ìƒì„±ëœ íŒŒì¼ë“¤ ì¶œë ¥
            print(f"\nğŸ“‚ ìƒì„±ëœ íŒŒì¼ë“¤:")
            if basic_result['reports']['comprehensive_report']:
                print(f"   ğŸ“‹ ì¢…í•© ë³´ê³ ì„œ: {os.path.basename(basic_result['reports']['comprehensive_report'])}")
            
            viz = basic_result['visualizations']
            if viz.get('sentiment_plot'):
                print(f"   ğŸ“Š ê°ì„± íŠ¸ë Œë“œ: {os.path.basename(viz['sentiment_plot'])}")
            if viz.get('topic_dashboard_plot'):
                print(f"   ğŸ” í† í”½ ëŒ€ì‹œë³´ë“œ: {os.path.basename(viz['topic_dashboard_plot'])}")
            if viz.get('time_comparison_plot'):
                print(f"   ğŸ“ˆ ì‹œê°„ë³„ ë¹„êµ: {os.path.basename(viz['time_comparison_plot'])}")
            if viz.get('time_distribution_plot'):
                print(f"   ğŸ“Š ì‹œê°„ ë¶„í¬ ë¶„ì„: {os.path.basename(viz['time_distribution_plot'])}")
            if viz.get('overall_wordcloud'):
                print(f"   â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ: {os.path.basename(viz['overall_wordcloud'])}")
            if viz.get('time_group_wordclouds') and len(viz['time_group_wordclouds']) > 0:
                print(f"   â˜ï¸ ê¸°ê°„ë³„ ì›Œë“œí´ë¼ìš°ë“œ: {len(viz['time_group_wordclouds'])}ê°œ")
            if viz.get('dashboard'):
                print(f"   ğŸ“± ëŒ€ì‹œë³´ë“œ: {os.path.basename(viz['dashboard'])}")
            
            print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
            print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {config.OUTPUT_DIR}")
            
            # ì‚¬ìš©ë²• ì•ˆë‚´
            print(f"\nğŸ’¡ ì‚¬ìš©ë²•:")
            print(f"   ê¸°ë³¸ ë¶„ì„: uv run python main.py --basic-only")
            
            logger.info("ğŸ‰ YouTube Comments Analysis ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ë©”ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            print(traceback.format_exc())
            return
    else:
        # ì „ì²´ ë¶„ì„ ìˆ˜í–‰ (ê¸°ë³¸ + ê³ ê¸‰)
        print("ğŸ“Š ê¸°ë³¸ ë¶„ì„ ëª¨ë“œ (ê¸°ë³¸ê°’)")
        try:
            # 1. ì„¤ì • ê²€ì¦
            print("ğŸ”§ ì„¤ì • ê²€ì¦ ì¤‘...")
            if not validate_config():
                print("âŒ ì„¤ì • ê²€ì¦ ì‹¤íŒ¨")
                return
            
            config = AnalysisConfig()
            logger = setup_logging(config)
            
            logger.info("ğŸš€ YouTube Comments Analysis ì‹œì‘")
            logger.info(f"ğŸ“± ë””ë°”ì´ìŠ¤: {config.DEVICE}")
            logger.info(f"ğŸ¯ ë¶„ì„ ëª¨ë“œ: {config.ANALYSIS_MODE}")
            logger.info(f"ğŸ“„ ë¶„ì„ íŒŒì¼: {config.DATA_FILES['youtube_comments']}")
            
            # 2. ê¸°ë³¸ ë¶„ì„ ì‹¤í–‰
            print(f"\nğŸ¯ {config.TARGET_NAME} ê¸°ë³¸ ë¶„ì„ ì‹œì‘...")
            
            basic_result = run_basic_analysis(config, logger)
            
            if not basic_result:
                print("âŒ ê¸°ë³¸ ë¶„ì„ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return
            
            print(f"âœ… {config.TARGET_NAME} ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ")
            
            # ì£¼ìš” ê²°ê³¼ ì¶œë ¥
            total_comments = basic_result['processed_data']['total_comments']
            date_range = basic_result['processed_data']['date_range']
            
            print(f"   ğŸ“Š ì´ ëŒ“ê¸€ ìˆ˜: {total_comments:,}ê°œ")
            if date_range['start'] and date_range['end']:
                print(f"   ğŸ“… ë¶„ì„ ê¸°ê°„: {date_range['start'].strftime('%Y-%m-%d')} ~ {date_range['end'].strftime('%Y-%m-%d')}")
            
            # ê°ì„± ë¶„ì„ ìš”ì•½
            if basic_result['sentiment_trends']:
                months = basic_result['sentiment_trends']['months']
                if months:
                    avg_positive = sum(basic_result['sentiment_trends']['positive_ratios']) / len(months)
                    avg_negative = sum(basic_result['sentiment_trends']['negative_ratios']) / len(months)
                    print(f"   ğŸ˜Š í‰ê·  ê¸ì • ë¹„ìœ¨: {avg_positive:.1%}")
                    print(f"   ğŸ˜ í‰ê·  ë¶€ì • ë¹„ìœ¨: {avg_negative:.1%}")
            
            # í† í”½ ë¶„ì„ ìš”ì•½
            if basic_result['topic_results']:
                bertopic_months = len([m for m, r in basic_result['topic_results'].items() 
                                     if r.get('bertopic') and r['bertopic'].get('topic_labels')])
                lda_months = len([m for m, r in basic_result['topic_results'].items() 
                                if r.get('lda') and r['lda'].get('topic_labels')])
                print(f"   ğŸ” BERTopic ë¶„ì„ ì™„ë£Œ: {bertopic_months}ê°œì›”")
                print(f"   ğŸ“š LDA ë¶„ì„ ì™„ë£Œ: {lda_months}ê°œì›”")
            
            # 3. ê³ ê¸‰ ë¶„ì„ ì‹¤í–‰ (ì˜µì…˜)
            advanced_result = None
            print(f"\nğŸš€ {config.TARGET_NAME} ê³ ê¸‰ ë¶„ì„ ì‹œì‘...")
            advanced_result = run_advanced_analysis(config, logger, basic_result)
            
            if advanced_result:
                print(f"âœ… {config.TARGET_NAME} ê³ ê¸‰ ë¶„ì„ ì™„ë£Œ")
                print(f"   ğŸ” ì‹œê°„ ê¸°ë°˜ ì—¬ë¡  íë¦„ ë¶„ì„ ì™„ë£Œ")
                print(f"   ğŸ“Š ì¢…í•© í† í”½ ë¶„ì„ ì™„ë£Œ")
                print(f"   ğŸ•¸ï¸ í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì™„ë£Œ")
                print(f"   ğŸ“ˆ ë³€ê³¡ì  íƒì§€ ì™„ë£Œ")
                print(f"   ğŸ§  ë¬¸ë§¥ ì„ë² ë”© ë¶„ì„ ì™„ë£Œ")
            else:
                print("âš ï¸ ê³ ê¸‰ ë¶„ì„ì´ ì‹¤íŒ¨í–ˆì§€ë§Œ ê¸°ë³¸ ë¶„ì„ì€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # 4. ìƒì„±ëœ íŒŒì¼ë“¤ ì¶œë ¥
            print(f"\nğŸ“‚ ìƒì„±ëœ íŒŒì¼ë“¤:")
            if basic_result['reports']['comprehensive_report']:
                print(f"   ğŸ“‹ ì¢…í•© ë³´ê³ ì„œ: {os.path.basename(basic_result['reports']['comprehensive_report'])}")
            
            viz = basic_result['visualizations']
            if viz.get('sentiment_plot'):
                print(f"   ğŸ“Š ê°ì„± íŠ¸ë Œë“œ: {os.path.basename(viz['sentiment_plot'])}")
            if viz.get('topic_dashboard_plot'):
                print(f"   ğŸ” í† í”½ ëŒ€ì‹œë³´ë“œ: {os.path.basename(viz['topic_dashboard_plot'])}")
            if viz.get('time_comparison_plot'):
                print(f"   ğŸ“ˆ ì‹œê°„ë³„ ë¹„êµ: {os.path.basename(viz['time_comparison_plot'])}")
            if viz.get('time_distribution_plot'):
                print(f"   ğŸ“Š ì‹œê°„ ë¶„í¬ ë¶„ì„: {os.path.basename(viz['time_distribution_plot'])}")
            if viz.get('overall_wordcloud'):
                print(f"   â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ: {os.path.basename(viz['overall_wordcloud'])}")
            if viz.get('time_group_wordclouds') and len(viz['time_group_wordclouds']) > 0:
                print(f"   â˜ï¸ ê¸°ê°„ë³„ ì›Œë“œí´ë¼ìš°ë“œ: {len(viz['time_group_wordclouds'])}ê°œ")
            if viz.get('dashboard'):
                print(f"   ğŸ“± ëŒ€ì‹œë³´ë“œ: {os.path.basename(viz['dashboard'])}")
            
            # ê³ ê¸‰ ë¶„ì„ ê²°ê³¼ íŒŒì¼ë“¤
            if advanced_result:
                adv_viz = advanced_result['advanced_visualizations']
                if adv_viz.get('temporal_flow_plot'):
                    print(f"   ğŸ“Š ì‹œê°„ë³„ ì—¬ë¡  íë¦„: {os.path.basename(adv_viz['temporal_flow_plot'])}")
                if adv_viz.get('topic_evolution_plot'):
                    print(f"   ğŸ” í† í”½ ì§„í™”: {os.path.basename(adv_viz['topic_evolution_plot'])}")
                if adv_viz.get('network_plot'):
                    print(f"   ğŸ•¸ï¸ í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬: {os.path.basename(adv_viz['network_plot'])}")
                if adv_viz.get('comprehensive_dashboard'):
                    print(f"   ğŸ“± ì¢…í•© ëŒ€ì‹œë³´ë“œ: {os.path.basename(adv_viz['comprehensive_dashboard'])}")
            
            print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
            print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {config.OUTPUT_DIR}")
            
            # ì‚¬ìš©ë²• ì•ˆë‚´
            print(f"\nğŸ’¡ ì‚¬ìš©ë²•:")
            print(f"   ê¸°ë³¸ ë¶„ì„: uv run python main.py --basic-only")
            
            logger.info("ğŸ‰ YouTube Comments Analysis ì™„ë£Œ")
            
            # 8. ì¢…í•© ê²°ê³¼ ì¤€ë¹„
            comprehensive_results = {
                'basic_analysis': basic_result,
                'advanced_analysis': advanced_result,
                'target_name': config.TARGET_NAME
            }
            
            # ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•œ ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ (comprehensive_results í˜•íƒœë¡œ ë³€í™˜)
            basic_analysis_for_report = basic_result['comprehensive_results']
            
            # 9. ì—°êµ¬ ëª©ì  íŠ¹í™” ë¶„ì„ (ë‰´ìŠ¤ vs ëŒ€ì¤‘ ë‹´ë¡  ì°¨ì´)
            print(f"\nğŸ”¬ {config.TARGET_NAME} ì—°êµ¬ ëª©ì  íŠ¹í™” ë¶„ì„...")
            
            # visualizer ê°ì²´ ìƒì„±
            from src.visualizer import Visualizer
            visualizer = Visualizer(config)
            
            # í”„ë ˆì„ ë³€í™” ì†ë„ ë¶„ì„
            try:
                frame_velocity_plot = visualizer.plot_frame_velocity_analysis(
                    basic_result['topic_results'], config.TARGET_NAME
                )
                if frame_velocity_plot:
                    print(f"   âœ… í”„ë ˆì„ ë³€í™” ì†ë„ ë¶„ì„: {os.path.basename(frame_velocity_plot)}")
            except Exception as e:
                logger.warning(f"âš ï¸ í”„ë ˆì„ ë³€í™” ì†ë„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            
            # ìˆ¨ê²¨ì§„ í•˜ìœ„ ë‹´ë¡  ë°œêµ´ ë¶„ì„
            try:
                hidden_discourse_plot = visualizer.plot_hidden_discourse_analysis(
                    basic_result['topic_results'], config.TARGET_NAME
                )
                if hidden_discourse_plot:
                    print(f"   âœ… ìˆ¨ê²¨ì§„ í•˜ìœ„ ë‹´ë¡  ë¶„ì„: {os.path.basename(hidden_discourse_plot)}")
            except Exception as e:
                logger.warning(f"âš ï¸ ìˆ¨ê²¨ì§„ í•˜ìœ„ ë‹´ë¡  ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            
            # ê°ì • ê°•ë„ ë¶„ì„ (ê¸°ì¡´)
            try:
                emotion_intensity_plot = visualizer.plot_emotion_intensity_analysis(
                    basic_result['topic_results'], config.TARGET_NAME
                )
                if emotion_intensity_plot:
                    print(f"   âœ… ê°ì • ê°•ë„ ë¶„ì„: {os.path.basename(emotion_intensity_plot)}")
            except Exception as e:
                logger.warning(f"âš ï¸ ê°ì • ê°•ë„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            
            # 10. ì¢…í•© ë³´ê³ ì„œ ìƒì„±
            logger.info(f"ğŸ“‹ {config.TARGET_NAME} ë³´ê³ ì„œ ìƒì„± ì‹œì‘")
            from src.report_generator import ReportGenerator
            report_generator = ReportGenerator(config)
            report_path = report_generator.generate_comprehensive_report(basic_analysis_for_report, config.TARGET_NAME)
            
            # 11. ì¸í„°ë™í‹°ë¸Œ ëŒ€ì‹œë³´ë“œ ìƒì„±
            dashboard_path = visualizer.create_interactive_dashboard(basic_analysis_for_report, config.TARGET_NAME)
            
            logger.info(f"âœ… {config.TARGET_NAME} ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ë©”ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            print(traceback.format_exc())
            return

if __name__ == "__main__":
    main() 