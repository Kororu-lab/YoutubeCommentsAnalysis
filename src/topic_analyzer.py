"""
Topic Analyzer Module
í† í”½ ë¶„ì„ ëª¨ë“ˆ
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional
import re
from collections import Counter

# BERTopic ê´€ë ¨
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.feature_extraction.text import CountVectorizer

# LDA ê´€ë ¨
from gensim import corpora, models
from gensim.models import LdaModel
from gensim.parsing.preprocessing import STOPWORDS
import pyLDAvis.gensim_models as gensimvis

# í•œêµ­ì–´ ì²˜ë¦¬
from konlpy.tag import Okt, Mecab
import warnings
warnings.filterwarnings('ignore')

# í•œêµ­ì–´ í† í¬ë‚˜ì´ì € í´ë˜ìŠ¤
class KoreanTokenizer:
    """í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í† í¬ë‚˜ì´ì €"""
    
    def __init__(self, use_mecab=True):
        """
        ì´ˆê¸°í™”
        Args:
            use_mecab: Mecab ì‚¬ìš© ì—¬ë¶€ (Falseë©´ Okt ì‚¬ìš©)
        """
        self.tokenizer_name = "Okt"  # ê¸°ë³¸ê°’ ì„¤ì •
        
        try:
            if use_mecab:
                self.tokenizer = Mecab()
                self.tokenizer_name = "Mecab"
                print(f"âœ… {self.tokenizer_name} í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì„±ê³µ")
            else:
                self.tokenizer = Okt()
                self.tokenizer_name = "Okt"
                print(f"âœ… {self.tokenizer_name} í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            print(f"âš ï¸ Mecab ì´ˆê¸°í™” ì‹¤íŒ¨, Oktë¡œ ëŒ€ì²´: {e}")
            self.tokenizer = Okt()
            self.tokenizer_name = "Okt"
            print(f"âœ… {self.tokenizer_name} í† í¬ë‚˜ì´ì €ë¡œ ëŒ€ì²´ ì™„ë£Œ")
    
    def __call__(self, text):
        """
        í…ìŠ¤íŠ¸ë¥¼ í† í°í™”
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
        Returns:
            í† í° ë¦¬ìŠ¤íŠ¸
        """
        try:
            if self.tokenizer_name == "Mecab":
                # Mecabì˜ ê²½ìš° í˜•íƒœì†Œ ë¶„ì„ í›„ ì˜ë¯¸ìˆëŠ” í’ˆì‚¬ë§Œ ì„ íƒ
                morphs = self.tokenizer.pos(text)
                tokens = []
                for word, pos in morphs:
                    # ëª…ì‚¬, í˜•ìš©ì‚¬, ë™ì‚¬ë§Œ ì„ íƒ
                    if pos.startswith(('NN', 'VA', 'VV')) and len(word) > 1:
                        tokens.append(word)
                return tokens
            else:
                # Oktì˜ ê²½ìš°
                return self.tokenizer.morphs(text, stem=True)
        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒì‹œ ê³µë°±ìœ¼ë¡œ ë¶„í• 
            return text.split()

class TopicAnalyzer:
    """í† í”½ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, config):
        """
        ì´ˆê¸°í™”
        Args:
            config: AnalysisConfig ê°ì²´
        """
        self.config = config
        self.logger = self._setup_logger()
        self.device = config.DEVICE
        
        # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°
        self.okt = Okt()
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.bertopic_model = None
        self.lda_model = None
        self.embedding_model = None
        
        # ê²°ê³¼ ì €ì¥
        self.results = {}
        
        self._initialize_models()
    
    def _setup_logger(self):
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger(__name__)
        logger.setLevel(getattr(logging, self.config.LOGGING['level']))
        
        if not logger.handlers:
            handler = logging.FileHandler(self.config.LOGGING['file'], encoding='utf-8')
            formatter = logging.Formatter(self.config.LOGGING['format'])
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
    
    def _initialize_models(self):
        """í† í”½ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            self.logger.info("ğŸ” í† í”½ ë¶„ì„ ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘")
            
            # BERTopicìš© í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸
            bertopic_config = self.config.TOPIC_MODELS['bertopic']
            self.logger.info(f"ğŸ“¥ í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {bertopic_config['embedding_model']}")
            
            self.embedding_model = SentenceTransformer(bertopic_config['embedding_model'])
            
            # BERTopic ëª¨ë¸ ì„¤ì • (í•œêµ­ì–´ ìµœì í™”)
            umap_model = UMAP(
                n_neighbors=15,
                n_components=5,
                min_dist=0.0,
                metric='cosine',
                random_state=42
            )
            
            hdbscan_model = HDBSCAN(
                min_cluster_size=bertopic_config['min_topic_size'],
                metric='euclidean',
                cluster_selection_method='eom',
                prediction_data=True
            )
            
            # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
            korean_tokenizer = KoreanTokenizer(use_mecab=True)
            
            # í•œêµ­ì–´ ë¶ˆìš©ì–´ ì„¤ì •
            korean_stopwords = set(self.config.KOREAN_STOPWORDS)
            
            # í•œêµ­ì–´ ìµœì í™” CountVectorizer (ë™ì  ì„¤ì •)
            vectorizer_model = CountVectorizer(
                tokenizer=korean_tokenizer,  # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš©
                ngram_range=(1, 2),
                stop_words=list(korean_stopwords),
                min_df=2,  # ìµœì†Œ ë¬¸ì„œ ë¹ˆë„ ê°ì†Œ
                max_df=0.95,  # ìµœëŒ€ ë¬¸ì„œ ë¹ˆë„ ì¦ê°€
                max_features=500,  # ìµœëŒ€ íŠ¹ì„± ìˆ˜ ê°ì†Œ
                lowercase=False  # í•œêµ­ì–´ëŠ” ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ
            )
            
            self.bertopic_model = BERTopic(
                embedding_model=self.embedding_model,
                umap_model=umap_model,
                hdbscan_model=hdbscan_model,
                vectorizer_model=vectorizer_model,
                language='korean',
                calculate_probabilities=True,
                verbose=True
            )
            
            self.logger.info("âœ… í•œêµ­ì–´ ìµœì í™” í† í”½ ë¶„ì„ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ í† í”½ ë¶„ì„ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise
    
    def preprocess_for_topic_analysis(self, texts: List[str]) -> List[str]:
        """
        í† í”½ ë¶„ì„ìš© í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        Args:
            texts: ì›ë³¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        Returns:
            ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        try:
            self.logger.info(f"ğŸ”§ í† í”½ ë¶„ì„ìš© í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹œì‘: {len(texts):,}ê°œ")
            
            processed_texts = []
            korean_stopwords = set(self.config.KOREAN_STOPWORDS)
            
            for text in texts:
                if not text or len(text.strip()) < 5:
                    processed_texts.append("")
                    continue
                
                try:
                    # í˜•íƒœì†Œ ë¶„ì„ ë° ëª…ì‚¬, í˜•ìš©ì‚¬ ì¶”ì¶œ
                    morphs = self.okt.pos(text, stem=True)
                    
                    # ì˜ë¯¸ìˆëŠ” í’ˆì‚¬ë§Œ ì„ íƒ (ëª…ì‚¬, í˜•ìš©ì‚¬, ë™ì‚¬)
                    meaningful_words = []
                    for word, pos in morphs:
                        if (pos in ['Noun', 'Adjective', 'Verb'] and 
                            len(word) > 1 and 
                            word not in korean_stopwords and
                            not word.isdigit() and
                            not re.match(r'^[ã„±-ã…ã…-ã…£]+$', word)):  # ììŒ/ëª¨ìŒë§Œ ìˆëŠ” ë‹¨ì–´ ì œì™¸
                            meaningful_words.append(word)
                    
                    # ìµœì†Œ ë‹¨ì–´ ìˆ˜ í™•ì¸
                    if len(meaningful_words) >= 2:
                        processed_texts.append(' '.join(meaningful_words))
                    else:
                        processed_texts.append("")
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    processed_texts.append("")
            
            # ë¹ˆ í…ìŠ¤íŠ¸ ì œê±°
            valid_texts = [text for text in processed_texts if text.strip()]
            
            self.logger.info(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(valid_texts):,}ê°œ ìœ íš¨ í…ìŠ¤íŠ¸")
            return processed_texts
            
        except Exception as e:
            self.logger.error(f"âŒ í† í”½ ë¶„ì„ìš© ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def analyze_bertopic(self, texts: List[str], target_name: str) -> Dict:
        """
        BERTopic ë¶„ì„
        Args:
            texts: ë¶„ì„í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            target_name: ë¶„ì„ ëŒ€ìƒ ì´ë¦„
        Returns:
            BERTopic ë¶„ì„ ê²°ê³¼
        """
        try:
            self.logger.info(f"ğŸ¯ {target_name} BERTopic ë¶„ì„ ì‹œì‘")
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            processed_texts = self.preprocess_for_topic_analysis(texts)
            
            # ë¹ˆ í…ìŠ¤íŠ¸ ì œê±°
            valid_texts = [text for text in processed_texts if text.strip()]
            valid_indices = [i for i, text in enumerate(processed_texts) if text.strip()]
            
            if len(valid_texts) < 10:
                self.logger.warning(f"âš ï¸ ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤: {len(valid_texts)}ê°œ")
                return {
                    'topics': [],
                    'topic_labels': [],
                    'document_topics': [],
                    'topic_words': {},
                    'topic_info': pd.DataFrame()
                }
            
            self.logger.info(f"ğŸ“Š BERTopic ëª¨ë¸ í•™ìŠµ ì¤‘... ({len(valid_texts):,}ê°œ ë¬¸ì„œ)")
            
            # ë¬¸ì„œ ìˆ˜ì— ë”°ë¥¸ ë™ì  íŒŒë¼ë¯¸í„° ì¡°ì •
            doc_count = len(valid_texts)
            
            # ë™ì  BERTopic ëª¨ë¸ ìƒì„± (ë” ë³´ìˆ˜ì ì¸ íŒŒë¼ë¯¸í„°)
            umap_model = UMAP(
                n_neighbors=min(8, max(2, doc_count // 20)),  # ë” ì‘ì€ ì´ì›ƒ ìˆ˜
                n_components=2,  # ê³ ì •ëœ ì°¨ì›
                min_dist=0.0,
                metric='cosine',
                random_state=42
            )
            
            hdbscan_model = HDBSCAN(
                min_cluster_size=max(2, min(4, doc_count // 40)),  # ë” ì‘ì€ í´ëŸ¬ìŠ¤í„° í¬ê¸°
                metric='euclidean',
                cluster_selection_method='eom',
                prediction_data=False  # ì˜ˆì¸¡ ë°ì´í„° ë¹„í™œì„±í™”
            )
            
            # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
            korean_tokenizer = KoreanTokenizer(use_mecab=True)
            
            # í•œêµ­ì–´ ë¶ˆìš©ì–´ ì„¤ì •
            korean_stopwords = set(self.config.KOREAN_STOPWORDS)
            
            # í•œêµ­ì–´ ìµœì í™” CountVectorizer (ë” ê°„ë‹¨í•œ ì„¤ì •)
            vectorizer_model = CountVectorizer(
                tokenizer=korean_tokenizer,  # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš©
                ngram_range=(1, 1),  # ë‹¨ì¼ ë‹¨ì–´ë§Œ ì‚¬ìš©
                stop_words=list(korean_stopwords),
                min_df=1,  # ìµœì†Œ ë¬¸ì„œ ë¹ˆë„ 1ë¡œ ì„¤ì •
                max_df=0.99,  # ìµœëŒ€ ë¬¸ì„œ ë¹ˆë„ 99%
                max_features=200,  # ìµœëŒ€ íŠ¹ì„± ìˆ˜ ë” ê°ì†Œ
                lowercase=False  # í•œêµ­ì–´ëŠ” ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ
            )
            
            dynamic_bertopic_model = BERTopic(
                embedding_model=self.embedding_model,
                umap_model=umap_model,
                hdbscan_model=hdbscan_model,
                vectorizer_model=vectorizer_model,
                language='korean',
                calculate_probabilities=False,  # í™•ë¥  ê³„ì‚° ë¹„í™œì„±í™”
                verbose=False  # ë¡œê·¸ ì¶œë ¥ ì¤„ì´ê¸°
            )
            
            # BERTopic ëª¨ë¸ í•™ìŠµ
            topics, probabilities = dynamic_bertopic_model.fit_transform(valid_texts)
            
            # í† í”½ ì •ë³´ ì¶”ì¶œ
            topic_info = dynamic_bertopic_model.get_topic_info()
            topic_words = {}
            topic_labels = []
            
            for topic_id in topic_info['Topic'].values:
                if topic_id != -1:  # ë…¸ì´ì¦ˆ í† í”½ ì œì™¸
                    words = dynamic_bertopic_model.get_topic(topic_id)
                    topic_words[topic_id] = words
                    
                    # í† í”½ ë¼ë²¨ ìƒì„± (ìƒìœ„ 3ê°œ ë‹¨ì–´)
                    top_words = [word for word, _ in words[:3]]
                    topic_labels.append(f"í† í”½{topic_id}: {', '.join(top_words)}")
            
            # ë¬¸ì„œë³„ í† í”½ í• ë‹¹ (ì›ë³¸ ì¸ë±ìŠ¤ì— ë§ì¶° ì¡°ì •)
            document_topics = [-1] * len(texts)
            for i, topic in enumerate(topics):
                if i < len(valid_indices):
                    document_topics[valid_indices[i]] = topic
            
            result = {
                'topics': topics,
                'topic_labels': topic_labels,
                'document_topics': document_topics,
                'topic_words': topic_words,
                'topic_info': topic_info,
                'probabilities': probabilities,
                'model': dynamic_bertopic_model
            }
            
            self.logger.info(f"âœ… BERTopic ë¶„ì„ ì™„ë£Œ: {len(topic_words)}ê°œ í† í”½ ë°œê²¬")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ BERTopic ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def find_optimal_topic_number(self, corpus, dictionary, documents, min_topics=2, max_topics=15):
        """
        ìµœì  í† í”½ ìˆ˜ ì°¾ê¸° (íœ´ë¦¬ìŠ¤í‹± ë°©ë²•)
        Args:
            corpus: ì½”í¼ìŠ¤
            dictionary: ì‚¬ì „
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            min_topics: ìµœì†Œ í† í”½ ìˆ˜
            max_topics: ìµœëŒ€ í† í”½ ìˆ˜
        Returns:
            ìµœì  í† í”½ ìˆ˜
        """
        try:
            # ë¬¸ì„œ ìˆ˜ì— ë”°ë¥¸ íœ´ë¦¬ìŠ¤í‹± ë°©ë²• (ë¹ ë¥´ê³  ì•ˆì •ì )
            doc_count = len(documents)
            
            if doc_count < 50:
                optimal_topics = min(max(doc_count // 15, 2), 4)
            elif doc_count < 100:
                optimal_topics = min(max(doc_count // 20, 3), 6)
            elif doc_count < 500:
                optimal_topics = min(max(doc_count // 25, 4), 8)
            else:
                optimal_topics = min(max(doc_count // 30, 5), 10)
            
            # ë²”ìœ„ ì œí•œ
            optimal_topics = max(min_topics, min(optimal_topics, max_topics))
            
            self.logger.info(f"ğŸ“Š íœ´ë¦¬ìŠ¤í‹± ìµœì  í† í”½ ìˆ˜: {optimal_topics} (ë¬¸ì„œ ìˆ˜: {doc_count})")
            return optimal_topics
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìµœì  í† í”½ ìˆ˜ ê³„ì‚° ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {str(e)}")
            return min_topics

    def analyze_lda(self, texts: List[str], target_name: str) -> Dict:
        """
        LDA í† í”½ ë¶„ì„ (ê°œì„ ëœ ë²„ì „)
        Args:
            texts: ë¶„ì„í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            target_name: ë¶„ì„ ëŒ€ìƒ ì´ë¦„
        Returns:
            LDA ë¶„ì„ ê²°ê³¼
        """
        try:
            self.logger.info(f"ğŸ“š {target_name} LDA ë¶„ì„ ì‹œì‘")
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            processed_texts = self.preprocess_for_topic_analysis(texts)
            
            # ë¹ˆ í…ìŠ¤íŠ¸ ì œê±° ë° í† í°í™”
            documents = []
            valid_indices = []
            
            for i, text in enumerate(processed_texts):
                if text.strip():
                    tokens = text.split()
                    if len(tokens) >= 2:  # ìµœì†Œ 2ê°œ ë‹¨ì–´
                        documents.append(tokens)
                        valid_indices.append(i)
            
            if len(documents) < 10:
                self.logger.warning(f"âš ï¸ LDA ë¶„ì„ìš© ìœ íš¨ ë¬¸ì„œê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤: {len(documents)}ê°œ")
                return {
                    'topics': [],
                    'topic_words': {},
                    'document_topics': [],
                    'coherence_score': 0.0,
                    'optimal_topic_count': 0
                }
            
            self.logger.info(f"ğŸ“Š LDA ëª¨ë¸ í•™ìŠµ ì¤‘... ({len(documents):,}ê°œ ë¬¸ì„œ)")
            
            # ì‚¬ì „ ë° ì½”í¼ìŠ¤ ìƒì„±
            dictionary = corpora.Dictionary(documents)
            
            # ë„ˆë¬´ ë¹ˆë²ˆí•˜ê±°ë‚˜ í¬ê·€í•œ ë‹¨ì–´ ì œê±°
            dictionary.filter_extremes(no_below=2, no_above=0.8)
            
            corpus = [dictionary.doc2bow(doc) for doc in documents]
            
            # ìµœì  í† í”½ ìˆ˜ ê²°ì •
            max_possible_topics = min(15, len(documents) // 5)
            if len(documents) >= 50:  # ì¶©ë¶„í•œ ë¬¸ì„œê°€ ìˆì„ ë•Œë§Œ ìµœì í™” ìˆ˜í–‰
                num_topics = self.find_optimal_topic_number(corpus, dictionary, documents, 
                                                          min_topics=2, max_topics=max_possible_topics)
            else:
                # ë¬¸ì„œê°€ ì ì„ ë•ŒëŠ” íœ´ë¦¬ìŠ¤í‹± ë°©ë²• ì‚¬ìš©
                num_topics = min(max(len(documents) // 10, 2), max_possible_topics)
            
            # LDA ëª¨ë¸ í•™ìŠµ (ë©”ëª¨ë¦¬ ì ˆì•½ ì„¤ì •)
            lda_config = self.config.TOPIC_MODELS['lda']
            
            # gensim ë²„ì „ì— ë”°ë¼ workers íŒŒë¼ë¯¸í„° ì§€ì› ì—¬ë¶€ê°€ ë‹¤ë¦„
            lda_params = {
                'corpus': corpus,
                'id2word': dictionary,
                'num_topics': num_topics,
                'random_state': lda_config.get('random_state', 42),
                'passes': lda_config.get('passes', 2),
                'alpha': lda_config.get('alpha', 'auto'),
                'eta': lda_config.get('eta', 'auto'),
                'per_word_topics': lda_config.get('per_word_topics', False),
                'chunksize': lda_config.get('chunksize', 200),
                'iterations': lda_config.get('iterations', 30),
                'eval_every': lda_config.get('eval_every', None)
            }
            
            # workers íŒŒë¼ë¯¸í„°ëŠ” gensim ë²„ì „ì— ë”°ë¼ ì§€ì›ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡°ê±´ë¶€ ì¶”ê°€
            try:
                # ë¨¼ì € workers íŒŒë¼ë¯¸í„° ì—†ì´ ì‹œë„
                self.lda_model = LdaModel(**lda_params)
            except Exception as e:
                self.logger.warning(f"âš ï¸ LDA ëª¨ë¸ ìƒì„± ì‹¤íŒ¨, ì¬ì‹œë„: {str(e)}")
                # íŒŒë¼ë¯¸í„°ë¥¼ ë” ë‹¨ìˆœí™”í•´ì„œ ì¬ì‹œë„
                simple_params = {
                    'corpus': corpus,
                    'id2word': dictionary,
                    'num_topics': num_topics,
                    'random_state': 42,
                    'passes': 2,
                    'alpha': 'auto',
                    'eta': 'auto'
                }
                self.lda_model = LdaModel(**simple_params)
            
            # í† í”½ë³„ ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ
            topic_words = {}
            topic_labels = []
            
            for topic_id in range(num_topics):
                words = self.lda_model.show_topic(topic_id, topn=10)
                topic_words[topic_id] = [(word, prob) for word, prob in words]
                
                # í† í”½ ë¼ë²¨ ìƒì„± (ë” ì˜ë¯¸ìˆëŠ” ë¼ë²¨)
                top_words = [word for word, _ in words[:3]]
                topic_labels.append(f"í† í”½{topic_id+1}: {', '.join(top_words)}")
            
            # ë¬¸ì„œë³„ ì£¼ìš” í† í”½ í• ë‹¹
            document_topics = [-1] * len(texts)
            
            for i, doc in enumerate(corpus):
                topic_probs = self.lda_model.get_document_topics(doc)
                if topic_probs:
                    # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í† í”½ ì„ íƒ
                    main_topic = max(topic_probs, key=lambda x: x[1])[0]
                    document_topics[valid_indices[i]] = main_topic
            
            # ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „ìœ¼ë¡œ ëŒ€ì²´)
            try:
                coherence_score = self._calculate_simple_coherence(topic_words, documents)
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
                coherence_score = 0.0
            
            # í† í”½ í’ˆì§ˆ í‰ê°€ ì¶”ê°€
            topic_quality = self._evaluate_topic_quality(topic_words, documents)
            
            result = {
                'topics': list(range(num_topics)),
                'topic_words': topic_words,
                'topic_labels': topic_labels,
                'document_topics': document_topics,
                'coherence_score': coherence_score,
                'optimal_topic_count': num_topics,
                'topic_quality': topic_quality,
                'model': self.lda_model,
                'dictionary': dictionary,
                'corpus': corpus
            }
            
            self.logger.info(f"âœ… LDA ë¶„ì„ ì™„ë£Œ: {num_topics}ê°œ í† í”½, ì¼ê´€ì„±: {coherence_score:.3f}, í’ˆì§ˆ: {topic_quality:.3f}")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ LDA ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _calculate_simple_coherence(self, topic_words: Dict, documents: List[List[str]]) -> float:
        """
        ë§¤ìš° ê°„ë‹¨í•œ í† í”½ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
        Args:
            topic_words: í† í”½ë³„ ë‹¨ì–´ ë”•ì…”ë„ˆë¦¬
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        Returns:
            ì¼ê´€ì„± ì ìˆ˜ (0-1)
        """
        try:
            if not topic_words or not documents:
                return 0.0
            
            # í† í”½ë³„ ë‹¨ì–´ ë‹¤ì–‘ì„±ë§Œ ê°„ë‹¨íˆ ê³„ì‚°
            topic_scores = []
            
            for topic_id, words in topic_words.items():
                if len(words) < 2:
                    continue
                
                # ìƒìœ„ 3ê°œ ë‹¨ì–´ë§Œ ì‚¬ìš© (ë” ê°„ë‹¨í•˜ê²Œ)
                top_words = [word for word, _ in words[:3]]
                
                # ë‹¨ì–´ë“¤ì´ ë¬¸ì„œì— ë‚˜íƒ€ë‚˜ëŠ” ë¹ˆë„ ê³„ì‚°
                word_doc_counts = []
                for word in top_words:
                    count = sum(1 for doc in documents if word in doc)
                    word_doc_counts.append(count)
                
                # í‰ê·  ë¬¸ì„œ ì¶œí˜„ ë¹ˆë„ë¥¼ ì¼ê´€ì„± ì ìˆ˜ë¡œ ì‚¬ìš©
                if word_doc_counts:
                    avg_frequency = np.mean(word_doc_counts) / len(documents)
                    topic_scores.append(min(avg_frequency, 1.0))
            
            return np.mean(topic_scores) if topic_scores else 0.0
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê°„ë‹¨í•œ ì¼ê´€ì„± ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return 0.0

    def _evaluate_topic_quality(self, topic_words: Dict, documents: List[List[str]]) -> float:
        """
        í† í”½ í’ˆì§ˆ í‰ê°€
        Args:
            topic_words: í† í”½ë³„ ë‹¨ì–´ ë”•ì…”ë„ˆë¦¬
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        Returns:
            í’ˆì§ˆ ì ìˆ˜ (0-1)
        """
        try:
            # í† í”½ ê°„ ë‹¨ì–´ ì¤‘ë³µë„ ê³„ì‚°
            all_topic_words = set()
            topic_word_sets = []
            
            for topic_id, words in topic_words.items():
                topic_set = set([word for word, _ in words[:5]])  # ìƒìœ„ 5ê°œ ë‹¨ì–´
                topic_word_sets.append(topic_set)
                all_topic_words.update(topic_set)
            
            # ì¤‘ë³µë„ê°€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
            total_words = len(all_topic_words)
            unique_words = sum(len(topic_set) for topic_set in topic_word_sets)
            
            if unique_words == 0:
                return 0.0
            
            diversity_score = total_words / unique_words
            
            # í† í”½ ë‚´ ë‹¨ì–´ ì‘ì§‘ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)
            coherence_scores = []
            for topic_set in topic_word_sets:
                if len(topic_set) > 1:
                    # í† í”½ ë‹¨ì–´ë“¤ì´ ê°™ì€ ë¬¸ì„œì— í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” ë¹ˆë„
                    co_occurrence = 0
                    total_pairs = 0
                    
                    for doc in documents:
                        doc_set = set(doc)
                        topic_in_doc = topic_set.intersection(doc_set)
                        if len(topic_in_doc) > 1:
                            co_occurrence += len(topic_in_doc) * (len(topic_in_doc) - 1) / 2
                        total_pairs += len(topic_set) * (len(topic_set) - 1) / 2
                    
                    if total_pairs > 0:
                        coherence_scores.append(co_occurrence / total_pairs)
            
            coherence_score = np.mean(coherence_scores) if coherence_scores else 0.0
            
            # ìµœì¢… í’ˆì§ˆ ì ìˆ˜ (ë‹¤ì–‘ì„±ê³¼ ì‘ì§‘ë„ì˜ ì¡°í™”í‰ê· )
            if diversity_score > 0 and coherence_score > 0:
                quality_score = 2 * diversity_score * coherence_score / (diversity_score + coherence_score)
            else:
                quality_score = 0.0
            
            return min(quality_score, 1.0)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í† í”½ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {str(e)}")
            return 0.0
    
    def analyze_monthly_topics(self, monthly_data: Dict[str, pd.DataFrame], target_name: str) -> Dict:
        """
        ì›”ë³„ í† í”½ ë¶„ì„
        Args:
            monthly_data: ì›”ë³„ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            target_name: ë¶„ì„ ëŒ€ìƒ ì´ë¦„
        Returns:
            ì›”ë³„ í† í”½ ë¶„ì„ ê²°ê³¼
        """
        try:
            self.logger.info(f"ğŸ“… {target_name} ì›”ë³„ í† í”½ ë¶„ì„ ì‹œì‘")
            
            monthly_results = {}
            
            for year_month, df in monthly_data.items():
                self.logger.info(f"ğŸ“Š {year_month} í† í”½ ë¶„ì„ ì¤‘... ({len(df):,}ê°œ ëŒ“ê¸€)")
                
                if len(df) < 10:  # ìµœì†Œ ë¬¸ì„œ ìˆ˜ í™•ì¸
                    self.logger.warning(f"âš ï¸ {year_month}: ë¬¸ì„œ ìˆ˜ê°€ ë„ˆë¬´ ì ì–´ í† í”½ ë¶„ì„ ìƒëµ")
                    continue
                
                texts = df['cleaned_text'].tolist()
                
                # BERTopic ë¶„ì„
                try:
                    bertopic_result = self.analyze_bertopic(texts, f"{target_name}_{year_month}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {year_month} BERTopic ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                    bertopic_result = {}
                
                # LDA ë¶„ì„
                try:
                    lda_result = self.analyze_lda(texts, f"{target_name}_{year_month}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {year_month} LDA ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                    lda_result = {}
                
                monthly_results[year_month] = {
                    'total_comments': len(df),
                    'bertopic': bertopic_result,
                    'lda': lda_result
                }
                
                # ì£¼ìš” í† í”½ ë¡œê¹…
                if bertopic_result and 'topic_labels' in bertopic_result:
                    self.logger.info(f"  BERTopic: {len(bertopic_result['topic_labels'])}ê°œ í† í”½")
                if lda_result and 'topic_labels' in lda_result:
                    self.logger.info(f"  LDA: {len(lda_result['topic_labels'])}ê°œ í† í”½")
            
            self.results[target_name] = monthly_results
            
            self.logger.info(f"âœ… {target_name} ì›”ë³„ í† í”½ ë¶„ì„ ì™„ë£Œ")
            return monthly_results
            
        except Exception as e:
            self.logger.error(f"âŒ {target_name} ì›”ë³„ í† í”½ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def analyze_time_grouped_topics(self, time_groups: Dict[str, pd.DataFrame], target_name: str) -> Dict:
        """
        ì ì‘ì  ì‹œê°„ ê·¸ë£¹ë³„ í† í”½ ë¶„ì„
        Args:
            time_groups: ì‹œê°„ ê·¸ë£¹ë³„ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            target_name: ë¶„ì„ ëŒ€ìƒ ì´ë¦„
        Returns:
            ì‹œê°„ ê·¸ë£¹ë³„ í† í”½ ë¶„ì„ ê²°ê³¼
        """
        try:
            self.logger.info(f"ğŸ“… {target_name} ì‹œê°„ ê·¸ë£¹ë³„ í† í”½ ë¶„ì„ ì‹œì‘")
            
            time_group_results = {}
            
            for group_name, df in time_groups.items():
                self.logger.info(f"ğŸ“Š {group_name} í† í”½ ë¶„ì„ ì¤‘... ({len(df):,}ê°œ ëŒ“ê¸€)")
                
                if len(df) < 10:  # ìµœì†Œ ë¬¸ì„œ ìˆ˜ í™•ì¸
                    self.logger.warning(f"âš ï¸ {group_name}: ë¬¸ì„œ ìˆ˜ê°€ ë„ˆë¬´ ì ì–´ í† í”½ ë¶„ì„ ìƒëµ")
                    continue
                
                texts = df['cleaned_text'].tolist()
                
                # BERTopic ë¶„ì„
                try:
                    bertopic_result = self.analyze_bertopic(texts, f"{target_name}_{group_name}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {group_name} BERTopic ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                    bertopic_result = {}
                
                # LDA ë¶„ì„
                try:
                    lda_result = self.analyze_lda(texts, f"{target_name}_{group_name}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {group_name} LDA ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                    lda_result = {}
                
                time_group_results[group_name] = {
                    'total_comments': len(df),
                    'bertopic': bertopic_result,
                    'lda': lda_result
                }
                
                # ì£¼ìš” í† í”½ ë¡œê¹…
                if bertopic_result and 'topic_labels' in bertopic_result:
                    self.logger.info(f"  BERTopic: {len(bertopic_result['topic_labels'])}ê°œ í† í”½")
                if lda_result and 'topic_labels' in lda_result:
                    self.logger.info(f"  LDA: {len(lda_result['topic_labels'])}ê°œ í† í”½")
            
            self.results[target_name] = time_group_results
            
            self.logger.info(f"âœ… {target_name} ì‹œê°„ ê·¸ë£¹ë³„ í† í”½ ë¶„ì„ ì™„ë£Œ")
            return time_group_results
            
        except Exception as e:
            self.logger.error(f"âŒ {target_name} ì‹œê°„ ê·¸ë£¹ë³„ í† í”½ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def get_topic_evolution(self, target_name: str) -> Dict:
        """
        í† í”½ ì§„í™” ë¶„ì„
        Args:
            target_name: ë¶„ì„ ëŒ€ìƒ ì´ë¦„
        Returns:
            í† í”½ ì§„í™” ë¶„ì„ ê²°ê³¼
        """
        try:
            if target_name not in self.results:
                raise ValueError(f"{target_name}ì˜ í† í”½ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            monthly_results = self.results[target_name]
            sorted_months = sorted(monthly_results.keys())
            
            # BERTopic ì§„í™”
            bertopic_evolution = {
                'months': sorted_months,
                'topic_counts': [],
                'main_topics': [],
                'topic_words_evolution': {}
            }
            
            # LDA ì§„í™”
            lda_evolution = {
                'months': sorted_months,
                'topic_counts': [],
                'main_topics': [],
                'coherence_scores': []
            }
            
            for month in sorted_months:
                result = monthly_results[month]
                
                # BERTopic ì§„í™” ì¶”ì 
                if 'bertopic' in result and result['bertopic']:
                    bertopic_data = result['bertopic']
                    bertopic_evolution['topic_counts'].append(len(bertopic_data.get('topic_labels', [])))
                    
                    if bertopic_data.get('topic_labels'):
                        bertopic_evolution['main_topics'].append(bertopic_data['topic_labels'][0])
                    else:
                        bertopic_evolution['main_topics'].append('ì—†ìŒ')
                else:
                    bertopic_evolution['topic_counts'].append(0)
                    bertopic_evolution['main_topics'].append('ì—†ìŒ')
                
                # LDA ì§„í™” ì¶”ì 
                if 'lda' in result and result['lda']:
                    lda_data = result['lda']
                    lda_evolution['topic_counts'].append(len(lda_data.get('topic_labels', [])))
                    lda_evolution['coherence_scores'].append(lda_data.get('coherence_score', 0.0))
                    
                    if lda_data.get('topic_labels'):
                        lda_evolution['main_topics'].append(lda_data['topic_labels'][0])
                    else:
                        lda_evolution['main_topics'].append('ì—†ìŒ')
                else:
                    lda_evolution['topic_counts'].append(0)
                    lda_evolution['coherence_scores'].append(0.0)
                    lda_evolution['main_topics'].append('ì—†ìŒ')
            
            return {
                'bertopic_evolution': bertopic_evolution,
                'lda_evolution': lda_evolution
            }
            
        except Exception as e:
            self.logger.error(f"âŒ {target_name} í† í”½ ì§„í™” ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def extract_keywords(self, texts: List[str], top_k: int = 20) -> List[Tuple[str, int]]:
        """
        í‚¤ì›Œë“œ ì¶”ì¶œ
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            top_k: ìƒìœ„ í‚¤ì›Œë“œ ê°œìˆ˜
        Returns:
            (í‚¤ì›Œë“œ, ë¹ˆë„) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            self.logger.info(f"ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘: {len(texts):,}ê°œ í…ìŠ¤íŠ¸")
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            processed_texts = self.preprocess_for_topic_analysis(texts)
            
            # ëª¨ë“  ë‹¨ì–´ ìˆ˜ì§‘
            all_words = []
            for text in processed_texts:
                if text.strip():
                    all_words.extend(text.split())
            
            # ë¹ˆë„ ê³„ì‚°
            word_counts = Counter(all_words)
            
            # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
            top_keywords = word_counts.most_common(top_k)
            
            self.logger.info(f"âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {len(top_keywords)}ê°œ")
            return top_keywords
            
        except Exception as e:
            self.logger.error(f"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            raise 