"""
Data Processor Module
ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ
"""

import pandas as pd
import numpy as np
import re
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import pickle
import os

class DataProcessor:
    """ìœ íŠœë¸Œ ëŒ“ê¸€ ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config):
        """
        ì´ˆê¸°í™”
        Args:
            config: AnalysisConfig ê°ì²´
        """
        self.config = config
        self.logger = self._setup_logger()
        self.raw_data = None
        self.processed_data = {}
        
    def _setup_logger(self):
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger(__name__)
        logger.setLevel(getattr(logging, self.config.LOGGING['level']))
        
        if not logger.handlers:
            handler = logging.FileHandler(self.config.LOGGING['file'], encoding='utf-8')
            formatter = logging.Formatter(self.config.LOGGING['format'])
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
    
    def load_data(self, file_path: str) -> pd.DataFrame:
        """
        ë°ì´í„° ë¡œë“œ
        Args:
            file_path: CSV íŒŒì¼ ê²½ë¡œ
        Returns:
            ë¡œë“œëœ ë°ì´í„°í”„ë ˆì„
        """
        try:
            self.logger.info(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì‹œì‘: {file_path}")
            
            # CSV íŒŒì¼ ë¡œë“œ (ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„)
            encodings = ['utf-8', 'cp949', 'euc-kr']
            for encoding in encodings:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    self.logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ (ì¸ì½”ë”©: {encoding})")
                    break
                except UnicodeDecodeError:
                    continue
            else:
                raise ValueError("ì§€ì›ë˜ëŠ” ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ê¸°ë³¸ ì •ë³´ ì¶œë ¥
            self.logger.info(f"ğŸ“Š ì´ ëŒ“ê¸€ ìˆ˜: {len(df):,}")
            self.logger.info(f"ğŸ“Š ì»¬ëŸ¼: {list(df.columns)}")
            
            self.raw_data = df
            return df
            
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def clean_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ë¶ˆìš©ì–´ ì œê±°
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
        Returns:
            ì •ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        if pd.isna(text) or not isinstance(text, str):
            return ""
        
        # ê¸°ë³¸ ì •ë¦¬
        text = str(text).strip()
        
        # URL ì œê±°
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # ì´ë©”ì¼ ì œê±°
        text = re.sub(r'\S+@\S+', '', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ ìœ ì§€)
        text = re.sub(r'[^\w\sê°€-í£ã„±-ã…ã…-ã…£.,!?]', ' ', text)
        
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        # ì—°ì†ëœ ë¬¸ì¥ë¶€í˜¸ ì œê±°
        text = re.sub(r'[.,!?]{2,}', '.', text)
        
        return text.strip()
    
    def remove_stopwords(self, text: str) -> str:
        """
        ë¶ˆìš©ì–´ ì œê±°
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
        Returns:
            ë¶ˆìš©ì–´ê°€ ì œê±°ëœ í…ìŠ¤íŠ¸
        """
        if not text:
            return ""
        
        # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ì„ ìœ„í•œ ê°„ë‹¨í•œ í† í°í™”
        words = text.split()
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = set(self.config.KOREAN_STOPWORDS)
        filtered_words = []
        
        for word in words:
            # ë‹¨ì–´ ì •ë¦¬
            word = word.strip('.,!?()[]{}"\'-')
            
            # ê¸¸ì´ê°€ 1ì¸ ë‹¨ì–´ ì œê±° (ì¡°ì‚¬ ë“±)
            if len(word) <= 1:
                continue
                
            # ë¶ˆìš©ì–´ ì²´í¬
            if word.lower() not in stopwords and word not in stopwords:
                # ìˆ«ìë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë‹¨ì–´ ì œê±°
                if not word.isdigit():
                    filtered_words.append(word)
        
        return ' '.join(filtered_words)
    
    def extract_target_comments(self, df: pd.DataFrame, target_name: str) -> pd.DataFrame:
        """
        íŠ¹ì • ëŒ€ìƒ ê´€ë ¨ ëŒ“ê¸€ ì¶”ì¶œ
        Args:
            df: ì „ì²´ ë°ì´í„°í”„ë ˆì„
            target_name: ë¶„ì„ ëŒ€ìƒ ì´ë¦„ (ì˜ˆ: 'ìœ ì•„ì¸')
        Returns:
            í•´ë‹¹ ëŒ€ìƒ ê´€ë ¨ ëŒ“ê¸€ ë°ì´í„°í”„ë ˆì„
        """
        try:
            self.logger.info(f"ğŸ¯ {target_name} ê´€ë ¨ ëŒ“ê¸€ ì¶”ì¶œ ì‹œì‘")
            
            target_config = self.config.TARGETS[target_name]
            keywords = target_config['keywords']
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
            mask = pd.Series([False] * len(df))
            
            for keyword in keywords:
                # ëŒ“ê¸€ ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰
                if 'comment_text' in df.columns:
                    mask |= df['comment_text'].str.contains(keyword, case=False, na=False)
                elif 'text' in df.columns:
                    mask |= df['text'].str.contains(keyword, case=False, na=False)
                elif 'content' in df.columns:
                    mask |= df['content'].str.contains(keyword, case=False, na=False)
                
                # ë¹„ë””ì˜¤ ì œëª©ì—ì„œë„ ê²€ìƒ‰ (ìˆëŠ” ê²½ìš°)
                if 'video_title' in df.columns:
                    mask |= df['video_title'].str.contains(keyword, case=False, na=False)
                elif 'title' in df.columns:
                    mask |= df['title'].str.contains(keyword, case=False, na=False)
            
            target_df = df[mask].copy()
            
            self.logger.info(f"âœ… {target_name} ê´€ë ¨ ëŒ“ê¸€ {len(target_df):,}ê°œ ì¶”ì¶œ ({len(target_df)/len(df)*100:.2f}%)")
            
            return target_df
            
        except Exception as e:
            self.logger.error(f"âŒ {target_name} ëŒ“ê¸€ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def preprocess_comments(self, df: pd.DataFrame, target_name: str) -> pd.DataFrame:
        """
        ëŒ“ê¸€ ì „ì²˜ë¦¬
        Args:
            df: ëŒ“ê¸€ ë°ì´í„°í”„ë ˆì„
            target_name: ë¶„ì„ ëŒ€ìƒ ì´ë¦„
        Returns:
            ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        """
        try:
            self.logger.info(f"ğŸ”§ {target_name} ëŒ“ê¸€ ì „ì²˜ë¦¬ ì‹œì‘")
            
            processed_df = df.copy()
            
            # ëŒ“ê¸€ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ í†µì¼
            text_columns = ['comment_text', 'text', 'content', 'comment']
            text_col = None
            for col in text_columns:
                if col in processed_df.columns:
                    text_col = col
                    break
            
            if text_col is None:
                raise ValueError("ëŒ“ê¸€ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ë¶ˆìš©ì–´ ì œê±°
            processed_df['cleaned_text'] = processed_df[text_col].apply(self.clean_text)
            processed_df['filtered_text'] = processed_df['cleaned_text'].apply(self.remove_stopwords)
            
            # ë¹ˆ ëŒ“ê¸€ ì œê±°
            processed_df = processed_df[processed_df['cleaned_text'].str.len() > 0]
            
            # ê¸¸ì´ í•„í„°ë§
            min_len = self.config.ANALYSIS_PARAMS['min_comment_length']
            max_len = self.config.ANALYSIS_PARAMS['max_comment_length']
            
            processed_df = processed_df[
                (processed_df['cleaned_text'].str.len() >= min_len) &
                (processed_df['cleaned_text'].str.len() <= max_len)
            ]
            
            # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
            date_columns = ['video_date', 'timestamp', 'published_at', 'date', 'created_at']
            date_col = None
            for col in date_columns:
                if col in processed_df.columns:
                    date_col = col
                    break
            
            if date_col:
                self.logger.info(f"ğŸ“… ë‚ ì§œ ì»¬ëŸ¼ ì‚¬ìš©: {date_col}")
                processed_df['date'] = pd.to_datetime(processed_df[date_col], errors='coerce')
                
                # ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨í•œ í–‰ í™•ì¸
                invalid_dates = processed_df['date'].isna().sum()
                if invalid_dates > 0:
                    self.logger.warning(f"âš ï¸ ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨: {invalid_dates:,}ê°œ í–‰")
                
                # ìœ íš¨í•œ ë‚ ì§œê°€ ìˆëŠ” í–‰ë§Œ ìœ ì§€
                before_date_filter = len(processed_df)
                processed_df = processed_df.dropna(subset=['date'])
                after_date_filter = len(processed_df)
                
                self.logger.info(f"ğŸ“… ë‚ ì§œ í•„í„°ë§: {before_date_filter:,} â†’ {after_date_filter:,}ê°œ")
                
                # ì›”ë³„ ê·¸ë£¹ ì¶”ê°€
                processed_df['year_month'] = processed_df['date'].dt.strftime('%Y-%m')
                
                # ë‚ ì§œ ë²”ìœ„ í™•ì¸
                if len(processed_df) > 0:
                    min_date = processed_df['date'].min()
                    max_date = processed_df['date'].max()
                    self.logger.info(f"ğŸ“… ë‚ ì§œ ë²”ìœ„: {min_date.strftime('%Y-%m-%d')} ~ {max_date.strftime('%Y-%m-%d')}")
            else:
                self.logger.warning("âš ï¸ ë‚ ì§œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                # ë‚ ì§œê°€ ì—†ì–´ë„ ë¶„ì„í•  ìˆ˜ ìˆë„ë¡ ê¸°ë³¸ ì›” ì„¤ì •
                processed_df['year_month'] = '2024-01'
                processed_df['date'] = pd.to_datetime('2024-01-01')
            
            # ì¤‘ë³µ ì œê±°
            initial_count = len(processed_df)
            processed_df = processed_df.drop_duplicates(subset=['cleaned_text'])
            final_count = len(processed_df)
            
            self.logger.info(f"ğŸ—‘ï¸ ì¤‘ë³µ ì œê±°: {initial_count - final_count:,}ê°œ")
            self.logger.info(f"âœ… {target_name} ì „ì²˜ë¦¬ ì™„ë£Œ: {final_count:,}ê°œ ëŒ“ê¸€")
            
            return processed_df
            
        except Exception as e:
            self.logger.error(f"âŒ {target_name} ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def group_by_month(self, df: pd.DataFrame, target_name: str) -> Dict[str, pd.DataFrame]:
        """
        ì›”ë³„ ë°ì´í„° ê·¸ë£¹í™”
        Args:
            df: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
            target_name: ë¶„ì„ ëŒ€ìƒ ì´ë¦„
        Returns:
            ì›”ë³„ ë°ì´í„°í”„ë ˆì„ ë”•ì…”ë„ˆë¦¬
        """
        try:
            self.logger.info(f"ğŸ“… {target_name} ì›”ë³„ ê·¸ë£¹í™” ì‹œì‘")
            
            if 'year_month' not in df.columns:
                raise ValueError("year_month ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë‚ ì§œ ì „ì²˜ë¦¬ë¥¼ ë¨¼ì € ìˆ˜í–‰í•˜ì„¸ìš”.")
            
            monthly_data = {}
            
            # ì›”ë³„ ê·¸ë£¹í™”
            for year_month, group in df.groupby('year_month'):
                if len(group) > 0:  # ë¹ˆ ê·¸ë£¹ ì œì™¸
                    monthly_data[year_month] = group.copy()
            
            # ì›”ë³„ í†µê³„
            self.logger.info(f"ğŸ“Š {target_name} ì›”ë³„ ëŒ“ê¸€ ë¶„í¬:")
            for year_month in sorted(monthly_data.keys()):
                count = len(monthly_data[year_month])
                self.logger.info(f"  {year_month}: {count:,}ê°œ")
            
            return monthly_data
            
        except Exception as e:
            self.logger.error(f"âŒ {target_name} ì›”ë³„ ê·¸ë£¹í™” ì‹¤íŒ¨: {str(e)}")
            raise
    
    def calculate_video_relevance(self, df: pd.DataFrame, target_name: str) -> pd.DataFrame:
        """
        ë¹„ë””ì˜¤ ê´€ë ¨ì„± ê³„ì‚°
        Args:
            df: ëŒ“ê¸€ ë°ì´í„°í”„ë ˆì„
            target_name: ë¶„ì„ ëŒ€ìƒ ì´ë¦„
        Returns:
            ê´€ë ¨ì„± ì •ë³´ê°€ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
        """
        try:
            self.logger.info(f"ğŸ¬ {target_name} ë¹„ë””ì˜¤ ê´€ë ¨ì„± ê³„ì‚° ì‹œì‘")
            
            target_config = self.config.TARGETS[target_name]
            keywords = target_config['keywords']
            
            # ë¹„ë””ì˜¤ë³„ ê·¸ë£¹í™”
            video_col = None
            for col in ['video_id', 'video_title', 'title']:
                if col in df.columns:
                    video_col = col
                    break
            
            if video_col is None:
                self.logger.warning("âš ï¸ ë¹„ë””ì˜¤ ì‹ë³„ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                df['video_relevance'] = 'unknown'
                return df
            
            df_with_relevance = df.copy()
            
            # ë¹„ë””ì˜¤ë³„ ê´€ë ¨ì„± ê³„ì‚°
            for video_id, group in df.groupby(video_col):
                total_comments = len(group)
                
                # í‚¤ì›Œë“œ í¬í•¨ ëŒ“ê¸€ ìˆ˜ ê³„ì‚°
                keyword_mentions = 0
                for keyword in keywords:
                    keyword_mentions += group['cleaned_text'].str.contains(keyword, case=False, na=False).sum()
                
                # ì œëª©ì—ì„œ í‚¤ì›Œë“œ í™•ì¸
                title_relevance = False
                if 'video_title' in group.columns or 'title' in group.columns:
                    title_col = 'video_title' if 'video_title' in group.columns else 'title'
                    title = str(group[title_col].iloc[0]) if not group[title_col].empty else ""
                    for keyword in keywords:
                        if keyword.lower() in title.lower():
                            title_relevance = True
                            break
                
                # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                mention_ratio = keyword_mentions / total_comments if total_comments > 0 else 0
                
                # ê´€ë ¨ì„± ë¶„ë¥˜
                if title_relevance or mention_ratio > 0.3:
                    relevance = 'high'
                elif mention_ratio > 0.1:
                    relevance = 'medium'
                elif mention_ratio > 0:
                    relevance = 'low'
                else:
                    relevance = 'none'
                
                # ë°ì´í„°í”„ë ˆì„ì— ê´€ë ¨ì„± ì •ë³´ ì¶”ê°€
                mask = df_with_relevance[video_col] == video_id
                df_with_relevance.loc[mask, 'video_relevance'] = relevance
                df_with_relevance.loc[mask, 'mention_ratio'] = mention_ratio
                df_with_relevance.loc[mask, 'title_relevance'] = title_relevance
            
            # ê´€ë ¨ì„± ë¶„í¬ ë¡œê¹…
            relevance_counts = df_with_relevance['video_relevance'].value_counts()
            self.logger.info(f"ğŸ“Š {target_name} ë¹„ë””ì˜¤ ê´€ë ¨ì„± ë¶„í¬:")
            for relevance, count in relevance_counts.items():
                self.logger.info(f"  {relevance}: {count:,}ê°œ ({count/len(df_with_relevance)*100:.1f}%)")
            
            return df_with_relevance
            
        except Exception as e:
            self.logger.error(f"âŒ {target_name} ë¹„ë””ì˜¤ ê´€ë ¨ì„± ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            raise
    
    def process_target(self, target_name: str) -> Dict:
        """
        íŠ¹ì • ëŒ€ìƒì— ëŒ€í•œ ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        Args:
            target_name: ë¶„ì„ ëŒ€ìƒ ì´ë¦„
        Returns:
            ì²˜ë¦¬ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        try:
            self.logger.info(f"ğŸš€ {target_name} ì „ì²´ ì „ì²˜ë¦¬ ì‹œì‘")
            
            if self.raw_data is None:
                raise ValueError("ì›ë³¸ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # 1. ëŒ€ìƒ ê´€ë ¨ ëŒ“ê¸€ ì¶”ì¶œ
            target_comments = self.extract_target_comments(self.raw_data, target_name)
            
            # 2. ëŒ“ê¸€ ì „ì²˜ë¦¬
            processed_comments = self.preprocess_comments(target_comments, target_name)
            
            # 3. ë¹„ë””ì˜¤ ê´€ë ¨ì„± ê³„ì‚°
            comments_with_relevance = self.calculate_video_relevance(processed_comments, target_name)
            
            # 4. ì›”ë³„ ê·¸ë£¹í™”
            monthly_data = self.group_by_month(comments_with_relevance, target_name)
            
            # ê²°ê³¼ ì €ì¥
            result = {
                'target_name': target_name,
                'total_comments': len(comments_with_relevance),
                'processed_data': comments_with_relevance,
                'monthly_data': monthly_data,
                'date_range': {
                    'start': comments_with_relevance['date'].min() if 'date' in comments_with_relevance.columns else None,
                    'end': comments_with_relevance['date'].max() if 'date' in comments_with_relevance.columns else None
                }
            }
            
            self.processed_data[target_name] = result
            
            self.logger.info(f"âœ… {target_name} ì „ì²´ ì „ì²˜ë¦¬ ì™„ë£Œ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ {target_name} ì „ì²´ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def save_processed_data(self, file_path: str):
        """
        ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        Args:
            file_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        """
        try:
            self.logger.info(f"ğŸ’¾ ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥: {file_path}")
            
            with open(file_path, 'wb') as f:
                pickle.dump(self.processed_data, f)
            
            self.logger.info("âœ… ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def load_processed_data(self, file_path: str):
        """
        ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
        Args:
            file_path: ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ
        """
        try:
            self.logger.info(f"ğŸ“‚ ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë“œ: {file_path}")
            
            with open(file_path, 'rb') as f:
                self.processed_data = pickle.load(f)
            
            self.logger.info("âœ… ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def group_by_month_single(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """
        ë‹¨ì¼ íŒŒì¼ ë¶„ì„ìš© ì›”ë³„ ê·¸ë£¹í™”
        Args:
            df: ì „ì²´ ë°ì´í„°í”„ë ˆì„
        Returns:
            ì›”ë³„ë¡œ ê·¸ë£¹í™”ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        try:
            self.logger.info("ğŸ“… ë‹¨ì¼ íŒŒì¼ ì›”ë³„ ê·¸ë£¹í™” ì‹œì‘")
            
            # ë‚ ì§œ ì»¬ëŸ¼ ì°¾ê¸°
            date_columns = ['video_date', 'timestamp', 'published_at', 'date', 'created_at']
            date_col = None
            for col in date_columns:
                if col in df.columns:
                    date_col = col
                    break
            
            if date_col is None:
                self.logger.warning("ë‚ ì§œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                return {'ì „ì²´': df}
            
            # ë‚ ì§œ ë³€í™˜
            df_copy = df.copy()
            df_copy['date'] = pd.to_datetime(df_copy[date_col], errors='coerce')
            
            # ìœ íš¨í•œ ë‚ ì§œë§Œ í•„í„°ë§
            df_copy = df_copy.dropna(subset=['date'])
            
            if len(df_copy) == 0:
                self.logger.warning("ìœ íš¨í•œ ë‚ ì§œê°€ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {'ì „ì²´': df}
            
            # ì›”ë³„ ê·¸ë£¹í™”
            df_copy['year_month'] = df_copy['date'].dt.strftime('%Y-%m')
            monthly_groups = {}
            
            for month, group in df_copy.groupby('year_month'):
                monthly_groups[month] = group
                self.logger.info(f"ğŸ“Š {month}: {len(group):,}ê°œ ëŒ“ê¸€")
            
            self.logger.info(f"âœ… ì´ {len(monthly_groups)}ê°œì›” ë°ì´í„° ê·¸ë£¹í™” ì™„ë£Œ")
            return monthly_groups
            
        except Exception as e:
            self.logger.error(f"âŒ ì›”ë³„ ê·¸ë£¹í™” ì‹¤íŒ¨: {str(e)}")
            return {'ì „ì²´': df}
    
    def process_single_file(self, file_path: str) -> Dict:
        """
        ë‹¨ì¼ íŒŒì¼ ì „ì²´ ì²˜ë¦¬
        Args:
            file_path: CSV íŒŒì¼ ê²½ë¡œ
        Returns:
            ì²˜ë¦¬ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        try:
            self.logger.info("ğŸš€ ë‹¨ì¼ íŒŒì¼ ë¶„ì„ ì‹œì‘")
            
            # ë°ì´í„° ë¡œë“œ
            df = self.load_data(file_path)
            
            # ê¸°ë³¸ ì „ì²˜ë¦¬
            text_columns = ['comment_text', 'text', 'content', 'comment']
            text_col = None
            for col in text_columns:
                if col in df.columns:
                    text_col = col
                    break
            
            if text_col is None:
                raise ValueError("ëŒ“ê¸€ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ë¶ˆìš©ì–´ ì œê±°
            df['cleaned_text'] = df[text_col].apply(self.clean_text)
            df['filtered_text'] = df['cleaned_text'].apply(self.remove_stopwords)
            
            # ë¹ˆ ëŒ“ê¸€ ì œê±°
            df = df[df['cleaned_text'].str.len() > 0]
            df = df[df['filtered_text'].str.len() > 0]
            
            # ê¸¸ì´ í•„í„°ë§
            min_len = self.config.ANALYSIS_PARAMS['min_comment_length']
            max_len = self.config.ANALYSIS_PARAMS['max_comment_length']
            
            df = df[
                (df['cleaned_text'].str.len() >= min_len) &
                (df['cleaned_text'].str.len() <= max_len)
            ]
            
            self.logger.info(f"ğŸ“Š ì „ì²˜ë¦¬ í›„ ëŒ“ê¸€ ìˆ˜: {len(df):,}")
            
            # ì›”ë³„ ê·¸ë£¹í™”
            monthly_data = self.group_by_month_single(df)
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                'total_data': df,
                'monthly_data': monthly_data,
                'summary': {
                    'total_comments': len(df),
                    'months_count': len(monthly_data),
                    'date_range': self._get_date_range(df),
                    'avg_comments_per_month': len(df) / len(monthly_data) if monthly_data else 0
                }
            }
            
            self.logger.info("âœ… ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _get_date_range(self, df: pd.DataFrame) -> Dict:
        """
        ë°ì´í„°ì˜ ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
        Args:
            df: ë°ì´í„°í”„ë ˆì„
        Returns:
            ë‚ ì§œ ë²”ìœ„ ì •ë³´
        """
        try:
            date_columns = ['video_date', 'timestamp', 'published_at', 'date', 'created_at']
            date_col = None
            for col in date_columns:
                if col in df.columns:
                    date_col = col
                    break
            
            if date_col is None:
                return {'start': None, 'end': None, 'span_days': 0}
            
            dates = pd.to_datetime(df[date_col], errors='coerce').dropna()
            
            if len(dates) == 0:
                return {'start': None, 'end': None, 'span_days': 0}
            
            start_date = dates.min()
            end_date = dates.max()
            span_days = (end_date - start_date).days
            
            return {
                'start': start_date.strftime('%Y-%m-%d'),
                'end': end_date.strftime('%Y-%m-%d'),
                'span_days': span_days
            }
            
        except Exception as e:
            self.logger.error(f"ë‚ ì§œ ë²”ìœ„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return {'start': None, 'end': None, 'span_days': 0} 