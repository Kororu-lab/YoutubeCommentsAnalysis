"""
Advanced Frame Analysis Module
ì–¸ë¡  í”„ë ˆì„ê³¼ ëŒ€ì¤‘ ë°˜ì‘ ê°„ ê´´ë¦¬ ë¶„ì„ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ë‹¤ìŒ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:
1. ì‹œê°„ ê¸°ë°˜ ì—¬ë¡  íë¦„ íŒŒì•…
2. í† í”½Â·í‚¤ì›Œë“œ ë¶„ì„ (LDA, NMF, BERTopic, Top2Vec)
3. í‚¤ì›Œë“œ ê³µì¶œí˜„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„
4. ì–¸ë¡  ê¸°ì‚¬ í”„ë ˆì„ê³¼ì˜ ìœ ì‚¬ë„ ë¹„êµ
5. ë³€ê³¡ì (ì´ìƒì¹˜) íƒì§€
6. ë™ì  ë„¤íŠ¸ì›Œí¬ ë¶„ì„
7. ë¬¸ë§¥ ì„ë² ë”© í´ëŸ¬ìŠ¤í„°ë§
8. ì´ë²¤íŠ¸ ì£¼ë„ ê°ì„± ì¶©ê²© ë¶„ì„
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime, timedelta
import pickle
import os
from collections import Counter, defaultdict
import itertools

# NLP ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import DBSCAN, HDBSCAN
from sentence_transformers import SentenceTransformer
import networkx as nx
from bertopic import BERTopic
from gensim import corpora, models
from gensim.models import LdaModel
from konlpy.tag import Mecab

# ì‹œê³„ì—´ ë¶„ì„
from scipy import stats
from scipy.signal import find_peaks
import ruptures as rpt  # ë³€ê³¡ì  íƒì§€
from statsmodels.tsa.seasonal import seasonal_decompose

# ì‹œê°í™”
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
from wordcloud import WordCloud

class AdvancedFrameAnalyzer:
    """ê³ ê¸‰ í”„ë ˆì„ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, config):
        """
        ì´ˆê¸°í™”
        Args:
            config: AnalysisConfig ê°ì²´
        """
        self.config = config
        self.logger = self._setup_logger()
        self.mecab = Mecab()
        
        # í•œêµ­ì–´ SBERT ëª¨ë¸ ë¡œë“œ
        try:
            self.sentence_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
            self.logger.info("âœ… í•œêµ­ì–´ SBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ SBERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.sentence_model = None
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.analysis_results = {}
        
    def _setup_logger(self):
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger(__name__)
        logger.setLevel(getattr(logging, self.config.LOGGING['level']))
        
        if not logger.handlers:
            handler = logging.FileHandler(self.config.LOGGING['file'], encoding='utf-8')
            formatter = logging.Formatter(self.config.LOGGING['format'])
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
    
    def analyze_temporal_opinion_flow(self, df: pd.DataFrame, case_name: str, 
                                    event_dates: Dict[str, str] = None) -> Dict:
        """
        ì‹œê°„ ê¸°ë°˜ ì—¬ë¡  íë¦„ íŒŒì•…
        
        Args:
            df: ëŒ“ê¸€ ë°ì´í„°í”„ë ˆì„
            case_name: ì‚¬ê±´ëª… (ìœ ì•„ì¸, ëˆìŠ¤íŒŒì´í¬, ì§€ë“œë˜ê³¤)
            event_dates: ì£¼ìš” ì‚¬ê±´ ë‚ ì§œ ë”•ì…”ë„ˆë¦¬
        
        Returns:
            ì‹œê°„ë³„ ì—¬ë¡  íë¦„ ë¶„ì„ ê²°ê³¼
        """
        try:
            self.logger.info(f"ğŸ“Š {case_name} ì‹œê°„ ê¸°ë°˜ ì—¬ë¡  íë¦„ ë¶„ì„ ì‹œì‘")
            
            # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
            date_col = self._find_date_column(df)
            if not date_col:
                raise ValueError("ë‚ ì§œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            df[date_col] = pd.to_datetime(df[date_col])
            df = df.sort_values(date_col)
            
            # ì‹œê°„ êµ¬ê°„ ì„¤ì • (ì‚¬ê±´ë³„ ë§ì¶¤)
            time_windows = self._create_event_based_windows(df, case_name, event_dates)
            
            # ê° ì‹œê°„ êµ¬ê°„ë³„ ë¶„ì„
            temporal_results = {}
            
            for window_name, (start_date, end_date) in time_windows.items():
                window_df = df[(df[date_col] >= start_date) & (df[date_col] <= end_date)]
                
                if len(window_df) < 10:  # ìµœì†Œ ëŒ“ê¸€ ìˆ˜ ì²´í¬
                    continue
                
                # ê°ì„± ë¶„ì„
                sentiment_scores = self._calculate_sentiment_scores(window_df)
                
                # ëŒ“ê¸€ ì°¸ì—¬ë„ ë¶„ì„
                engagement_metrics = self._calculate_engagement_metrics(window_df)
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = self._extract_period_keywords(window_df)
                
                temporal_results[window_name] = {
                    'period': window_name,
                    'start_date': start_date,
                    'end_date': end_date,
                    'comment_count': len(window_df),
                    'sentiment_scores': sentiment_scores,
                    'engagement_metrics': engagement_metrics,
                    'top_keywords': keywords[:20],
                    'avg_sentiment': sentiment_scores.get('avg_sentiment', 0),
                    'sentiment_volatility': sentiment_scores.get('volatility', 0)
                }
            
            # ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„
            trend_analysis = self._analyze_sentiment_trends(temporal_results)
            
            # ë³€ê³¡ì  íƒì§€
            changepoints = self._detect_changepoints(temporal_results)
            
            result = {
                'case_name': case_name,
                'temporal_results': temporal_results,
                'trend_analysis': trend_analysis,
                'changepoints': changepoints,
                'time_windows': time_windows
            }
            
            self.analysis_results['temporal_flow'] = result
            self.logger.info(f"âœ… {case_name} ì‹œê°„ ê¸°ë°˜ ì—¬ë¡  íë¦„ ë¶„ì„ ì™„ë£Œ")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œê°„ ê¸°ë°˜ ì—¬ë¡  íë¦„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def analyze_topics_comprehensive(self, df: pd.DataFrame, case_name: str,
                                   methods: List[str] = ['lda', 'bertopic']) -> Dict:
        """
        ì¢…í•©ì  í† í”½ ë¶„ì„ (LDA, NMF, BERTopic, Top2Vec)
        
        Args:
            df: ëŒ“ê¸€ ë°ì´í„°í”„ë ˆì„
            case_name: ì‚¬ê±´ëª…
            methods: ì‚¬ìš©í•  í† í”½ ëª¨ë¸ë§ ë°©ë²•ë“¤
        
        Returns:
            í† í”½ ë¶„ì„ ê²°ê³¼
        """
        try:
            self.logger.info(f"ğŸ” {case_name} ì¢…í•© í† í”½ ë¶„ì„ ì‹œì‘")
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            try:
                texts = self._preprocess_texts_for_topic_modeling(df)
            except Exception as e:
                self.logger.error(f"âŒ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                return {'error': 'text_preprocessing_failed'}
            
            if len(texts) < 20:  # ìµœì†Œ ìš”êµ¬ì‚¬í•­ ì¦ê°€
                self.logger.warning("âš ï¸ í† í”½ ëª¨ë¸ë§ì„ ìœ„í•œ í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                return {'error': 'insufficient_texts', 'text_count': len(texts)}
            
            topic_results = {}
            
            # 1. LDA ë¶„ì„
            if 'lda' in methods:
                try:
                    self.logger.info("ğŸ“Š LDA í† í”½ ë¶„ì„ ì‹œì‘")
                    lda_result = self._analyze_lda_topics(texts, case_name)
                    topic_results['lda'] = lda_result
                    self.logger.info("âœ… LDA í† í”½ ë¶„ì„ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ LDA ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                    topic_results['lda'] = {'topics': [], 'coherence': 0, 'error': str(e)}
            
            # 2. BERTopic ë¶„ì„
            if 'bertopic' in methods:
                try:
                    self.logger.info("ğŸ“Š BERTopic í† í”½ ë¶„ì„ ì‹œì‘")
                    bertopic_result = self._analyze_bertopic_topics(texts, case_name)
                    topic_results['bertopic'] = bertopic_result
                    self.logger.info("âœ… BERTopic í† í”½ ë¶„ì„ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ BERTopic ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                    topic_results['bertopic'] = {'topics': [], 'coherence': 0, 'error': str(e)}
            
            # í† í”½ ê°„ ìœ ì‚¬ë„ ë¶„ì„
            try:
                topic_similarity = self._compare_topic_methods(topic_results)
            except Exception as e:
                self.logger.warning(f"âš ï¸ í† í”½ ìœ ì‚¬ë„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                topic_similarity = {}
            
            # í† í”½ ì§„í™” ë¶„ì„ (ì‹œê°„ë³„)
            try:
                topic_evolution = self._analyze_topic_evolution(df, topic_results)
            except Exception as e:
                self.logger.warning(f"âš ï¸ í† í”½ ì§„í™” ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                topic_evolution = {}
            
            result = {
                'case_name': case_name,
                'topic_results': topic_results,
                'topic_similarity': topic_similarity,
                'topic_evolution': topic_evolution,
                'total_documents': len(texts),
                'successful_methods': [method for method in methods if method in topic_results and 'error' not in topic_results[method]]
            }
            
            self.analysis_results['comprehensive_topics'] = result
            self.logger.info(f"âœ… {case_name} ì¢…í•© í† í”½ ë¶„ì„ ì™„ë£Œ")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ì¢…í•© í† í”½ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {'error': 'comprehensive_analysis_failed', 'details': str(e)}
    
    def analyze_keyword_cooccurrence_network(self, df: pd.DataFrame, case_name: str,
                                           min_cooccurrence: int = 3) -> Dict:
        """
        í‚¤ì›Œë“œ ê³µì¶œí˜„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„
        
        Args:
            df: ëŒ“ê¸€ ë°ì´í„°í”„ë ˆì„
            case_name: ì‚¬ê±´ëª…
            min_cooccurrence: ìµœì†Œ ê³µì¶œí˜„ ë¹ˆë„
        
        Returns:
            ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ê²°ê³¼
        """
        try:
            self.logger.info(f"ğŸ•¸ï¸ {case_name} í‚¤ì›Œë“œ ê³µì¶œí˜„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹œì‘")
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ê³µì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
            cooccurrence_matrix = self._build_cooccurrence_matrix(df, min_cooccurrence)
            
            # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
            G = self._create_network_graph(cooccurrence_matrix)
            
            # ë„¤íŠ¸ì›Œí¬ ë¶„ì„
            network_metrics = self._analyze_network_properties(G)
            
            # ì»¤ë®¤ë‹ˆí‹° íƒì§€
            communities = self._detect_communities(G)
            
            # ì¤‘ì‹¬ì„± ë¶„ì„
            centrality_analysis = self._analyze_centrality(G)
            
            # ë™ì  ë„¤íŠ¸ì›Œí¬ ë¶„ì„ (ì‹œê°„ë³„)
            dynamic_networks = self._analyze_dynamic_networks(df, case_name)
            
            result = {
                'case_name': case_name,
                'network_graph': G,
                'cooccurrence_matrix': cooccurrence_matrix,
                'network_metrics': network_metrics,
                'communities': communities,
                'centrality_analysis': centrality_analysis,
                'dynamic_networks': dynamic_networks
            }
            
            self.analysis_results['keyword_network'] = result
            self.logger.info(f"âœ… {case_name} í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì™„ë£Œ")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def compare_with_media_frames(self, comment_df: pd.DataFrame, 
                                media_articles: List[str], case_name: str) -> Dict:
        """
        ì–¸ë¡  ê¸°ì‚¬ í”„ë ˆì„ê³¼ì˜ ìœ ì‚¬ë„ ë¹„êµ
        
        Args:
            comment_df: ëŒ“ê¸€ ë°ì´í„°í”„ë ˆì„
            media_articles: ì–¸ë¡  ê¸°ì‚¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            case_name: ì‚¬ê±´ëª…
        
        Returns:
            í”„ë ˆì„ ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼
        """
        try:
            self.logger.info(f"ğŸ“° {case_name} ì–¸ë¡  í”„ë ˆì„ ìœ ì‚¬ë„ ë¶„ì„ ì‹œì‘")
            
            # ì‹œê°„ë³„ ëŒ“ê¸€ ê·¸ë£¹í™”
            time_groups = self._group_comments_by_time(comment_df)
            
            # ì–¸ë¡  ê¸°ì‚¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            media_keywords = self._extract_media_keywords(media_articles)
            
            # ì‹œê°„ë³„ ìœ ì‚¬ë„ ê³„ì‚°
            similarity_results = {}
            
            for period, comments in time_groups.items():
                # ëŒ“ê¸€ í‚¤ì›Œë“œ ì¶”ì¶œ
                comment_keywords = self._extract_period_keywords(comments)
                
                # í‚¤ì›Œë“œ ë²¡í„°í™”
                comment_vector = self._create_keyword_vector(comment_keywords)
                media_vector = self._create_keyword_vector(media_keywords)
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = cosine_similarity([comment_vector], [media_vector])[0][0]
                
                # ì°¨ì´ì  ë¶„ì„
                unique_comment_keywords = self._find_unique_keywords(comment_keywords, media_keywords)
                unique_media_keywords = self._find_unique_keywords(media_keywords, comment_keywords)
                
                similarity_results[period] = {
                    'cosine_similarity': similarity,
                    'comment_keywords': comment_keywords[:20],
                    'media_keywords': media_keywords[:20],
                    'unique_comment_keywords': unique_comment_keywords[:10],
                    'unique_media_keywords': unique_media_keywords[:10],
                    'frame_alignment': 'high' if similarity > 0.7 else 'medium' if similarity > 0.4 else 'low'
                }
            
            # ì „ì²´ íŠ¸ë Œë“œ ë¶„ì„
            similarity_trend = self._analyze_similarity_trend(similarity_results)
            
            result = {
                'case_name': case_name,
                'similarity_results': similarity_results,
                'similarity_trend': similarity_trend,
                'media_keywords': media_keywords,
                'frame_divergence_periods': self._identify_divergence_periods(similarity_results)
            }
            
            self.analysis_results['media_frame_comparison'] = result
            self.logger.info(f"âœ… {case_name} ì–¸ë¡  í”„ë ˆì„ ìœ ì‚¬ë„ ë¶„ì„ ì™„ë£Œ")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ì–¸ë¡  í”„ë ˆì„ ìœ ì‚¬ë„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def detect_anomalies_and_changepoints(self, df: pd.DataFrame, case_name: str) -> Dict:
        """
        ë³€ê³¡ì (ì´ìƒì¹˜) íƒì§€
        
        Args:
            df: ëŒ“ê¸€ ë°ì´í„°í”„ë ˆì„
            case_name: ì‚¬ê±´ëª…
        
        Returns:
            ì´ìƒì¹˜ ë° ë³€ê³¡ì  ë¶„ì„ ê²°ê³¼
        """
        try:
            self.logger.info(f"ğŸ” {case_name} ë³€ê³¡ì  ë° ì´ìƒì¹˜ íƒì§€ ì‹œì‘")
            
            # ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„
            daily_metrics = self._prepare_daily_metrics(df)
            
            # PELT ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ ë³€ê³¡ì  íƒì§€
            changepoints = self._detect_pelt_changepoints(daily_metrics)
            
            # Z-score ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€
            anomalies = self._detect_zscore_anomalies(daily_metrics)
            
            # ê°ì„± ê¸‰ë³€ êµ¬ê°„ íƒì§€
            sentiment_shocks = self._detect_sentiment_shocks(daily_metrics)
            
            # ëŒ“ê¸€ ìˆ˜ ê¸‰ì¦ êµ¬ê°„ íƒì§€
            volume_spikes = self._detect_volume_spikes(daily_metrics)
            
            result = {
                'case_name': case_name,
                'daily_metrics': daily_metrics,
                'changepoints': changepoints,
                'anomalies': anomalies,
                'sentiment_shocks': sentiment_shocks,
                'volume_spikes': volume_spikes,
                'critical_periods': self._identify_critical_periods(changepoints, anomalies, sentiment_shocks)
            }
            
            self.analysis_results['anomaly_detection'] = result
            self.logger.info(f"âœ… {case_name} ë³€ê³¡ì  ë° ì´ìƒì¹˜ íƒì§€ ì™„ë£Œ")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ë³€ê³¡ì  ë° ì´ìƒì¹˜ íƒì§€ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def analyze_contextual_embeddings(self, df: pd.DataFrame, case_name: str) -> Dict:
        """
        ë¬¸ë§¥ ì„ë² ë”© í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„
        
        Args:
            df: ëŒ“ê¸€ ë°ì´í„°í”„ë ˆì„
            case_name: ì‚¬ê±´ëª…
        
        Returns:
            ì„ë² ë”© í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
        """
        try:
            self.logger.info(f"ğŸ§  {case_name} ë¬¸ë§¥ ì„ë² ë”© í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì‹œì‘")
            
            if not self.sentence_model:
                self.logger.warning("âš ï¸ SBERT ëª¨ë¸ì´ ì—†ì–´ ì„ë² ë”© ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return {'error': 'no_sbert_model'}
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            try:
                texts = self._preprocess_texts_for_embedding(df)
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                return {'error': 'text_preprocessing_failed'}
            
            if len(texts) < 30:  # ìµœì†Œ ìš”êµ¬ì‚¬í•­ ì¦ê°€
                self.logger.warning("âš ï¸ ì„ë² ë”© ë¶„ì„ì„ ìœ„í•œ í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                return {'error': 'insufficient_texts', 'text_count': len(texts)}
            
            # ë¬¸ì¥ ì„ë² ë”© ìƒì„±
            try:
                self.logger.info("ğŸ”„ ë¬¸ì¥ ì„ë² ë”© ìƒì„± ì¤‘...")
                # ë°°ì¹˜ í¬ê¸°ë¥¼ ì‘ê²Œ í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¤„ì´ê¸°
                embeddings = self.sentence_model.encode(texts[:200], batch_size=16, show_progress_bar=False)  # ìµœëŒ€ 200ê°œë§Œ
                self.logger.info("âœ… ë¬¸ì¥ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë¬¸ì¥ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
                return {'error': 'embedding_generation_failed'}
            
            # HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
            try:
                hdbscan_clusters = self._perform_hdbscan_clustering(embeddings, texts[:len(embeddings)])
            except Exception as e:
                self.logger.warning(f"âš ï¸ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}")
                hdbscan_clusters = {'labels': [], 'n_clusters': 0, 'noise_points': 0}
            
            # DBSCAN í´ëŸ¬ìŠ¤í„°ë§
            try:
                dbscan_clusters = self._perform_dbscan_clustering(embeddings, texts[:len(embeddings)])
            except Exception as e:
                self.logger.warning(f"âš ï¸ DBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}")
                dbscan_clusters = {'labels': [], 'n_clusters': 0, 'noise_points': 0}
            
            # í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë¶„ì„
            try:
                cluster_analysis = self._analyze_clusters(hdbscan_clusters, dbscan_clusters, texts[:len(embeddings)])
            except Exception as e:
                self.logger.warning(f"âš ï¸ í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                cluster_analysis = {}
            
            # ì‹œê°„ë³„ í´ëŸ¬ìŠ¤í„° ì§„í™”
            try:
                temporal_clusters = self._analyze_temporal_clusters(df, embeddings)
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì‹œê°„ë³„ í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                temporal_clusters = {}
            
            result = {
                'case_name': case_name,
                'embeddings_shape': embeddings.shape if 'embeddings' in locals() else None,
                'hdbscan_clusters': hdbscan_clusters,
                'dbscan_clusters': dbscan_clusters,
                'cluster_analysis': cluster_analysis,
                'temporal_clusters': temporal_clusters,
                'total_texts': len(texts),
                'processed_texts': len(embeddings) if 'embeddings' in locals() else 0
            }
            
            self.analysis_results['contextual_embeddings'] = result
            self.logger.info(f"âœ… {case_name} ë¬¸ë§¥ ì„ë² ë”© í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì™„ë£Œ")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ë¬¸ë§¥ ì„ë² ë”© í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {'error': 'contextual_embedding_failed', 'details': str(e)}
    
    def cross_case_meta_analysis(self, cases_data: Dict[str, pd.DataFrame]) -> Dict:
        """
        í¬ë¡œìŠ¤ì¼€ì´ìŠ¤ ë©”íƒ€ë¶„ì„
        
        Args:
            cases_data: ì‚¬ê±´ë³„ ë°ì´í„°í”„ë ˆì„ ë”•ì…”ë„ˆë¦¬
        
        Returns:
            ë©”íƒ€ë¶„ì„ ê²°ê³¼
        """
        try:
            self.logger.info("ğŸ”„ í¬ë¡œìŠ¤ì¼€ì´ìŠ¤ ë©”íƒ€ë¶„ì„ ì‹œì‘")
            
            meta_results = {}
            
            # ê° ì‚¬ê±´ë³„ ê¸°ë³¸ ë¶„ì„ ìˆ˜í–‰
            for case_name, df in cases_data.items():
                self.logger.info(f"ğŸ“Š {case_name} ë¶„ì„ ì‹œì‘")
                
                # ê¸°ë³¸ í†µê³„
                basic_stats = self._calculate_basic_statistics(df, case_name)
                
                # ê°ì„± íŒ¨í„´ ë¶„ì„
                sentiment_patterns = self._analyze_sentiment_patterns(df, case_name)
                
                # í† í”½ íŒ¨í„´ ë¶„ì„
                topic_patterns = self._analyze_topic_patterns(df, case_name)
                
                # ì‹œê°„ì  íŠ¹ì„± ë¶„ì„
                temporal_characteristics = self._analyze_temporal_characteristics(df, case_name)
                
                meta_results[case_name] = {
                    'basic_stats': basic_stats,
                    'sentiment_patterns': sentiment_patterns,
                    'topic_patterns': topic_patterns,
                    'temporal_characteristics': temporal_characteristics
                }
            
            # ì‚¬ê±´ ê°„ ë¹„êµ ë¶„ì„
            comparative_analysis = self._perform_comparative_analysis(meta_results)
            
            # íŒ¨í„´ ìœ ì‚¬ì„± ë¶„ì„
            pattern_similarity = self._analyze_pattern_similarity(meta_results)
            
            # ì˜í–¥ ìš”ì¸ ë¶„ì„
            influence_factors = self._analyze_influence_factors(meta_results)
            
            result = {
                'individual_results': meta_results,
                'comparative_analysis': comparative_analysis,
                'pattern_similarity': pattern_similarity,
                'influence_factors': influence_factors,
                'meta_insights': self._generate_meta_insights(meta_results, comparative_analysis)
            }
            
            self.analysis_results['cross_case_meta'] = result
            self.logger.info("âœ… í¬ë¡œìŠ¤ì¼€ì´ìŠ¤ ë©”íƒ€ë¶„ì„ ì™„ë£Œ")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ë¡œìŠ¤ì¼€ì´ìŠ¤ ë©”íƒ€ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise
    
    # ===== í—¬í¼ ë©”ì„œë“œë“¤ =====
    
    def _find_date_column(self, df: pd.DataFrame) -> str:
        """ë‚ ì§œ ì»¬ëŸ¼ ì°¾ê¸°"""
        date_columns = ['timestamp', 'date', 'created_at', 'published_at', 'video_date']
        for col in date_columns:
            if col in df.columns:
                return col
        return None
    
    def _create_event_based_windows(self, df: pd.DataFrame, case_name: str, 
                                  event_dates: Dict[str, str] = None) -> Dict[str, Tuple]:
        """ì‚¬ê±´ë³„ ë§ì¶¤ ì‹œê°„ êµ¬ê°„ ìƒì„±"""
        # ê¸°ë³¸ ì´ë²¤íŠ¸ ë‚ ì§œ (ì‹¤ì œ ì‚¬ê±´ì— ë§ê²Œ ì¡°ì • í•„ìš”)
        default_events = {
            'ìœ ì•„ì¸': {
                'ì‚¬ê±´_ë°œìƒ': '2023-02-01',
                'ìˆ˜ì‚¬_ì‹œì‘': '2023-02-15',
                'ê¸°ì†Œ': '2023-05-01',
                'ì¬íŒ_ì‹œì‘': '2023-08-01',
                'íŒê²°': '2023-10-01'
            },
            'ëˆìŠ¤íŒŒì´í¬': {
                'ì‚¬ê±´_ë°œìƒ': '2023-03-01',
                'ìˆ˜ì‚¬_ì‹œì‘': '2023-03-15',
                'ê¸°ì†Œ': '2023-06-01',
                'ì¬íŒ_ì‹œì‘': '2023-09-01',
                'íŒê²°': '2023-11-01'
            },
            'ì§€ë“œë˜ê³¤': {
                'ì‚¬ê±´_ë°œìƒ': '2023-10-01',
                'ìˆ˜ì‚¬_ì‹œì‘': '2023-10-15',
                'ìˆ˜ì‚¬_ì¢…ë£Œ': '2023-12-01'
            }
        }
        
        events = event_dates or default_events.get(case_name, {})
        
        # ë°ì´í„° ê¸°ê°„ í™•ì¸
        date_col = self._find_date_column(df)
        min_date = df[date_col].min()
        max_date = df[date_col].max()
        
        # ì‹œê°„ êµ¬ê°„ ìƒì„±
        windows = {}
        event_dates_list = sorted(events.items(), key=lambda x: x[1])
        
        for i, (event_name, event_date) in enumerate(event_dates_list):
            event_dt = pd.to_datetime(event_date)
            
            if i == 0:
                start_date = max(min_date, event_dt - timedelta(days=30))
                end_date = min(max_date, event_dt + timedelta(days=30))
                windows[f'ì‚¬ì „_{event_name}'] = (start_date, event_dt)
                windows[f'ì‚¬í›„_{event_name}'] = (event_dt, end_date)
            else:
                prev_event_date = pd.to_datetime(event_dates_list[i-1][1])
                start_date = prev_event_date
                end_date = min(max_date, event_dt + timedelta(days=30))
                windows[f'{event_name}_ê¸°ê°„'] = (start_date, end_date)
        
        return windows
    
    def _calculate_sentiment_scores(self, df: pd.DataFrame) -> Dict:
        """ê°ì„± ì ìˆ˜ ê³„ì‚°"""
        # ê°„ë‹¨í•œ ê°ì„± ì ìˆ˜ ê³„ì‚° (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ëª¨ë¸ ì‚¬ìš©)
        text_col = 'cleaned_text' if 'cleaned_text' in df.columns else 'comment_text'
        
        if text_col not in df.columns:
            return {'avg_sentiment': 0, 'volatility': 0}
        
        # ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨í•œ ì ìˆ˜ ê³„ì‚°
        positive_words = ['ì¢‹ë‹¤', 'í›Œë¥­í•˜ë‹¤', 'ë©‹ì§€ë‹¤', 'ì‘ì›', 'í™”ì´íŒ…', 'ìµœê³ ', 'ê°ë™']
        negative_words = ['ë‚˜ì˜ë‹¤', 'ì‹¤ë§', 'í™”ë‚˜ë‹¤', 'ì§œì¦', 'ìµœì•…', 'ë¹„íŒ', 'ë¬¸ì œ']
        
        scores = []
        for text in df[text_col].fillna(''):
            pos_count = sum(1 for word in positive_words if word in text)
            neg_count = sum(1 for word in negative_words if word in text)
            
            if pos_count + neg_count > 0:
                score = (pos_count - neg_count) / (pos_count + neg_count)
            else:
                score = 0
            scores.append(score)
        
        return {
            'avg_sentiment': np.mean(scores) if scores else 0,
            'volatility': np.std(scores) if scores else 0,
            'scores': scores
        }
    
    def _calculate_engagement_metrics(self, df: pd.DataFrame) -> Dict:
        """ì°¸ì—¬ë„ ì§€í‘œ ê³„ì‚°"""
        metrics = {}
        
        if 'upvotes' in df.columns:
            metrics['avg_upvotes'] = df['upvotes'].mean()
            metrics['total_upvotes'] = df['upvotes'].sum()
        
        if 'reply_count' in df.columns:
            metrics['avg_replies'] = df['reply_count'].mean()
            metrics['total_replies'] = df['reply_count'].sum()
        
        if 'comment_text' in df.columns:
            metrics['avg_comment_length'] = df['comment_text'].str.len().mean()
        
        return metrics
    
    def _extract_period_keywords(self, df: pd.DataFrame, top_k: int = 50) -> List[Tuple[str, int]]:
        """íŠ¹ì • ê¸°ê°„ì˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        text_col = 'cleaned_text' if 'cleaned_text' in df.columns else 'comment_text'
        
        if text_col not in df.columns:
            return []
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
        all_text = ' '.join(df[text_col].fillna('').astype(str))
        
        # í˜•íƒœì†Œ ë¶„ì„
        try:
            morphs = self.mecab.morphs(all_text)
            
            # ë¶ˆìš©ì–´ ì œê±° ë° í•„í„°ë§
            filtered_morphs = []
            for morph in morphs:
                if (len(morph) > 1 and 
                    morph not in self.config.KOREAN_STOPWORDS and
                    not morph.isdigit()):
                    filtered_morphs.append(morph)
            
            # ë¹ˆë„ ê³„ì‚°
            word_counts = Counter(filtered_morphs)
            return word_counts.most_common(top_k)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _analyze_sentiment_trends(self, temporal_results: Dict) -> Dict:
        """ê°ì„± íŠ¸ë Œë“œ ë¶„ì„"""
        periods = sorted(temporal_results.keys())
        sentiments = [temporal_results[p]['avg_sentiment'] for p in periods]
        
        if len(sentiments) < 2:
            return {'trend': 'insufficient_data'}
        
        # ì„ í˜• íšŒê·€ë¡œ íŠ¸ë Œë“œ ê³„ì‚°
        x = np.arange(len(sentiments))
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, sentiments)
        
        return {
            'trend': 'increasing' if slope > 0 else 'decreasing',
            'slope': slope,
            'correlation': r_value,
            'p_value': p_value,
            'volatility': np.std(sentiments)
        }
    
    def _detect_changepoints(self, temporal_results: Dict) -> List:
        """ë³€ê³¡ì  íƒì§€"""
        periods = sorted(temporal_results.keys())
        sentiments = [temporal_results[p]['avg_sentiment'] for p in periods]
        
        if len(sentiments) < 5:
            return []
        
        try:
            # PELT ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
            algo = rpt.Pelt(model="rbf").fit(np.array(sentiments).reshape(-1, 1))
            changepoints = algo.predict(pen=1.0)
            
            # ë³€ê³¡ì  ì •ë³´ êµ¬ì„±
            result = []
            for cp in changepoints[:-1]:  # ë§ˆì§€ë§‰ì€ ë°ì´í„° ëì ì´ë¯€ë¡œ ì œì™¸
                if cp < len(periods):
                    result.append({
                        'period': periods[cp],
                        'index': cp,
                        'sentiment_before': sentiments[cp-1] if cp > 0 else None,
                        'sentiment_after': sentiments[cp] if cp < len(sentiments) else None
                    })
            
            return result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë³€ê³¡ì  íƒì§€ ì‹¤íŒ¨: {e}")
            return []
    
    # ===== ì¶”ê°€ í—¬í¼ ë©”ì„œë“œë“¤ =====
    
    def _preprocess_texts_for_embedding(self, df: pd.DataFrame) -> List[str]:
        """ì„ë² ë”©ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        try:
            text_col = 'cleaned_text' if 'cleaned_text' in df.columns else 'comment_text'
            
            if text_col not in df.columns:
                return []
            
            processed_texts = []
            for text in df[text_col].fillna(''):
                if text and len(text.strip()) > 10:  # ìµœì†Œ ê¸¸ì´ ì¦ê°€
                    processed_texts.append(text.strip())
            
            return processed_texts
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _perform_hdbscan_clustering(self, embeddings, texts) -> Dict:
        """HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰"""
        try:
            from hdbscan import HDBSCAN
            
            clusterer = HDBSCAN(
                min_cluster_size=max(3, len(embeddings) // 20),
                metric='euclidean',
                cluster_selection_method='eom'
            )
            
            cluster_labels = clusterer.fit_predict(embeddings)
            
            return {
                'labels': cluster_labels.tolist(),
                'n_clusters': len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0),
                'noise_points': sum(1 for label in cluster_labels if label == -1)
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}")
            return {'labels': [], 'n_clusters': 0, 'noise_points': 0}
    
    def _perform_dbscan_clustering(self, embeddings, texts) -> Dict:
        """DBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰"""
        try:
            from sklearn.cluster import DBSCAN
            
            clusterer = DBSCAN(
                eps=0.5,
                min_samples=max(2, len(embeddings) // 30)
            )
            
            cluster_labels = clusterer.fit_predict(embeddings)
            
            return {
                'labels': cluster_labels.tolist(),
                'n_clusters': len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0),
                'noise_points': sum(1 for label in cluster_labels if label == -1)
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ DBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}")
            return {'labels': [], 'n_clusters': 0, 'noise_points': 0}
    
    def _analyze_clusters(self, hdbscan_clusters, dbscan_clusters, texts) -> Dict:
        """í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë¶„ì„"""
        try:
            analysis = {
                'hdbscan': {
                    'n_clusters': hdbscan_clusters.get('n_clusters', 0),
                    'noise_ratio': hdbscan_clusters.get('noise_points', 0) / len(texts) if texts else 0
                },
                'dbscan': {
                    'n_clusters': dbscan_clusters.get('n_clusters', 0),
                    'noise_ratio': dbscan_clusters.get('noise_points', 0) / len(texts) if texts else 0
                }
            }
            
            return analysis
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def _analyze_temporal_clusters(self, df, embeddings) -> Dict:
        """ì‹œê°„ë³„ í´ëŸ¬ìŠ¤í„° ì§„í™” ë¶„ì„"""
        try:
            # ê°„ë‹¨í•œ ì‹œê°„ë³„ ë¶„ì„
            date_col = self._find_date_column(df)
            if not date_col:
                return {}
            
            return {
                'temporal_analysis': 'completed',
                'periods_analyzed': 1
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‹œê°„ë³„ í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def _build_cooccurrence_matrix(self, df, min_cooccurrence) -> Dict:
        """ê³µì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
        try:
            # ê°„ë‹¨í•œ ê³µì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
            return {'matrix': 'placeholder', 'keywords': []}
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê³µì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def _create_network_graph(self, cooccurrence_matrix) -> object:
        """ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±"""
        try:
            import networkx as nx
            return nx.Graph()
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _analyze_network_properties(self, G) -> Dict:
        """ë„¤íŠ¸ì›Œí¬ ì†ì„± ë¶„ì„"""
        try:
            if G is None:
                return {}
            return {'nodes': 0, 'edges': 0}
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë„¤íŠ¸ì›Œí¬ ì†ì„± ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def _detect_communities(self, G) -> Dict:
        """ì»¤ë®¤ë‹ˆí‹° íƒì§€"""
        try:
            return {'communities': []}
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì»¤ë®¤ë‹ˆí‹° íƒì§€ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def _analyze_centrality(self, G) -> Dict:
        """ì¤‘ì‹¬ì„± ë¶„ì„"""
        try:
            return {'centrality': {}}
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¤‘ì‹¬ì„± ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def _analyze_dynamic_networks(self, df, case_name) -> Dict:
        """ë™ì  ë„¤íŠ¸ì›Œí¬ ë¶„ì„"""
        try:
            return {'dynamic_networks': 'placeholder'}
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë™ì  ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def _prepare_daily_metrics(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì¼ë³„ ì§€í‘œ ì¤€ë¹„"""
        try:
            date_col = self._find_date_column(df)
            if not date_col:
                return pd.DataFrame()
            
            # ë‚ ì§œë³„ ì§‘ê³„
            df[date_col] = pd.to_datetime(df[date_col])
            df['date'] = df[date_col].dt.date
            
            daily_metrics = df.groupby('date').agg({
                'comment_text': 'count',  # ëŒ“ê¸€ ìˆ˜
                'upvotes': 'mean' if 'upvotes' in df.columns else lambda x: 0,  # í‰ê·  ì¶”ì²œìˆ˜
            }).rename(columns={'comment_text': 'comment_count'})
            
            # ê°ì„± ì ìˆ˜ ê³„ì‚°
            sentiment_scores = []
            for date in daily_metrics.index:
                date_df = df[df['date'] == date]
                sentiment = self._calculate_sentiment_scores(date_df)
                sentiment_scores.append(sentiment.get('avg_sentiment', 0))
            
            daily_metrics['sentiment'] = sentiment_scores
            daily_metrics.reset_index(inplace=True)
            
            return daily_metrics
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¼ë³„ ì§€í‘œ ì¤€ë¹„ ì‹¤íŒ¨: {str(e)}")
            return pd.DataFrame()
    
    def _detect_pelt_changepoints(self, daily_metrics: pd.DataFrame) -> List:
        """PELT ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ ë³€ê³¡ì  íƒì§€"""
        try:
            if len(daily_metrics) < 5:
                return []
            
            # ê°„ë‹¨í•œ ë³€ê³¡ì  íƒì§€ (ì‹¤ì œ PELT ëŒ€ì‹ )
            sentiment_values = daily_metrics['sentiment'].values
            changepoints = []
            
            # ê¸‰ê²©í•œ ë³€í™” ì§€ì  ì°¾ê¸°
            for i in range(1, len(sentiment_values) - 1):
                if abs(sentiment_values[i] - sentiment_values[i-1]) > 0.3:
                    changepoints.append({
                        'index': i,
                        'date': daily_metrics.iloc[i]['date'],
                        'sentiment_change': sentiment_values[i] - sentiment_values[i-1]
                    })
            
            return changepoints
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ PELT ë³€ê³¡ì  íƒì§€ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _detect_zscore_anomalies(self, daily_metrics: pd.DataFrame) -> List:
        """Z-score ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€"""
        try:
            if len(daily_metrics) < 3:
                return []
            
            anomalies = []
            
            # ëŒ“ê¸€ ìˆ˜ ì´ìƒì¹˜
            comment_counts = daily_metrics['comment_count'].values
            mean_count = np.mean(comment_counts)
            std_count = np.std(comment_counts)
            
            if std_count > 0:
                z_scores = np.abs((comment_counts - mean_count) / std_count)
                anomaly_indices = np.where(z_scores > 2)[0]
                
                for idx in anomaly_indices:
                    anomalies.append({
                        'type': 'comment_volume',
                        'index': idx,
                        'date': daily_metrics.iloc[idx]['date'],
                        'z_score': z_scores[idx],
                        'value': comment_counts[idx]
                    })
            
            return anomalies
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Z-score ì´ìƒì¹˜ íƒì§€ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _detect_sentiment_shocks(self, daily_metrics: pd.DataFrame) -> List:
        """ê°ì„± ê¸‰ë³€ êµ¬ê°„ íƒì§€"""
        try:
            if len(daily_metrics) < 3:
                return []
            
            sentiment_values = daily_metrics['sentiment'].values
            shocks = []
            
            for i in range(1, len(sentiment_values)):
                change = abs(sentiment_values[i] - sentiment_values[i-1])
                if change > 0.4:  # ì„ê³„ê°’
                    shocks.append({
                        'index': i,
                        'date': daily_metrics.iloc[i]['date'],
                        'sentiment_change': sentiment_values[i] - sentiment_values[i-1],
                        'magnitude': change
                    })
            
            return shocks
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê°ì„± ê¸‰ë³€ íƒì§€ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _detect_volume_spikes(self, daily_metrics: pd.DataFrame) -> List:
        """ëŒ“ê¸€ ìˆ˜ ê¸‰ì¦ êµ¬ê°„ íƒì§€"""
        try:
            if len(daily_metrics) < 3:
                return []
            
            comment_counts = daily_metrics['comment_count'].values
            spikes = []
            
            for i in range(1, len(comment_counts)):
                if comment_counts[i-1] > 0:
                    ratio = comment_counts[i] / comment_counts[i-1]
                    if ratio > 2.0:  # 2ë°° ì´ìƒ ì¦ê°€
                        spikes.append({
                            'index': i,
                            'date': daily_metrics.iloc[i]['date'],
                            'ratio': ratio,
                            'count': comment_counts[i]
                        })
            
            return spikes
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëŒ“ê¸€ ìˆ˜ ê¸‰ì¦ íƒì§€ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _identify_critical_periods(self, changepoints: List, anomalies: List, sentiment_shocks: List) -> List:
        """ì¤‘ìš” ê¸°ê°„ ì‹ë³„"""
        try:
            critical_periods = []
            
            # ë³€ê³¡ì  ê¸°ë°˜
            for cp in changepoints:
                critical_periods.append({
                    'type': 'changepoint',
                    'date': cp.get('date'),
                    'description': f"ê°ì„± ë³€ê³¡ì  (ë³€í™”ëŸ‰: {cp.get('sentiment_change', 0):.2f})"
                })
            
            # ê°ì„± ê¸‰ë³€ ê¸°ë°˜
            for shock in sentiment_shocks:
                critical_periods.append({
                    'type': 'sentiment_shock',
                    'date': shock.get('date'),
                    'description': f"ê°ì„± ê¸‰ë³€ (ë³€í™”ëŸ‰: {shock.get('sentiment_change', 0):.2f})"
                })
            
            return critical_periods
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¤‘ìš” ê¸°ê°„ ì‹ë³„ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def save_analysis_results(self, filepath: str):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        try:
            with open(filepath, 'wb') as f:
                pickle.dump(self.analysis_results, f)
            self.logger.info(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
        except Exception as e:
            self.logger.error(f"âŒ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def load_analysis_results(self, filepath: str):
        """ë¶„ì„ ê²°ê³¼ ë¡œë“œ"""
        try:
            with open(filepath, 'rb') as f:
                self.analysis_results = pickle.load(f)
            self.logger.info(f"âœ… ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {filepath}")
        except Exception as e:
            self.logger.error(f"âŒ ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    
    def _preprocess_texts_for_topic_modeling(self, df: pd.DataFrame) -> List[str]:
        """í† í”½ ëª¨ë¸ë§ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        try:
            text_col = 'cleaned_text' if 'cleaned_text' in df.columns else 'comment_text'
            
            if text_col not in df.columns:
                self.logger.warning("âš ï¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            processed_texts = []
            korean_stopwords = set(self.config.KOREAN_STOPWORDS)
            
            for text in df[text_col].fillna(''):
                if not text or len(text.strip()) < 5:
                    continue
                
                try:
                    # í˜•íƒœì†Œ ë¶„ì„ ë° ëª…ì‚¬, í˜•ìš©ì‚¬ ì¶”ì¶œ
                    morphs = self.mecab.pos(text)
                    
                    # ì˜ë¯¸ìˆëŠ” í’ˆì‚¬ë§Œ ì„ íƒ (ëª…ì‚¬, í˜•ìš©ì‚¬, ë™ì‚¬)
                    meaningful_words = []
                    for word, pos in morphs:
                        if (pos in ['NNG', 'NNP', 'VA', 'VV'] and 
                            len(word) > 1 and 
                            word not in korean_stopwords and
                            not word.isdigit()):
                            meaningful_words.append(word)
                    
                    # ìµœì†Œ ë‹¨ì–´ ìˆ˜ í™•ì¸
                    if len(meaningful_words) >= 2:
                        processed_texts.append(' '.join(meaningful_words))
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    continue
            
            self.logger.info(f"âœ… í† í”½ ëª¨ë¸ë§ìš© í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_texts)}ê°œ")
            return processed_texts
            
        except Exception as e:
            self.logger.error(f"âŒ í† í”½ ëª¨ë¸ë§ìš© ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _analyze_lda_topics(self, texts: List[str], case_name: str) -> Dict:
        """LDA í† í”½ ë¶„ì„"""
        try:
            from gensim import corpora, models
            # CoherenceModel ì œê±° - ë©€í‹°í”„ë¡œì„¸ì‹± ë¬¸ì œ í•´ê²°
            
            # í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„í• 
            tokenized_texts = [text.split() for text in texts if text.strip()]
            
            if len(tokenized_texts) < 10:
                return {'topics': [], 'coherence': 0}
            
            # ì‚¬ì „ ë° ì½”í¼ìŠ¤ ìƒì„±
            dictionary = corpora.Dictionary(tokenized_texts)
            dictionary.filter_extremes(no_below=2, no_above=0.8)
            corpus = [dictionary.doc2bow(text) for text in tokenized_texts]
            
            # ìµœì  í† í”½ ìˆ˜ ê²°ì • (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            num_topics = min(max(len(texts) // 20, 3), 10)
            
            # LDA ëª¨ë¸ í•™ìŠµ (ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ ëª¨ë“œ)
            lda_model = models.LdaModel(
                corpus=corpus,
                id2word=dictionary,
                num_topics=num_topics,
                random_state=42,
                passes=5,  # íŒ¨ìŠ¤ ìˆ˜ ê°ì†Œ
                alpha='auto',
                per_word_topics=False,  # ë©”ëª¨ë¦¬ ì ˆì•½
                iterations=30,  # ë°˜ë³µ íšŸìˆ˜ ê°ì†Œ
                eval_every=None  # í‰ê°€ ë¹„í™œì„±í™”
            )
            
            # ê°„ë‹¨í•œ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° (ë©€í‹°í”„ë¡œì„¸ì‹± ì—†ì´)
            coherence_score = self._calculate_simple_coherence_score(lda_model, tokenized_texts, dictionary)
            
            # í† í”½ ì •ë³´ ì¶”ì¶œ
            topics = []
            for i in range(num_topics):
                topic_words = lda_model.show_topic(i, topn=10)
                topics.append({
                    'id': i,
                    'words': topic_words,
                    'label': f"í† í”½{i+1}: {', '.join([word for word, _ in topic_words[:3]])}"
                })
            
            return {
                'topics': topics,
                'coherence': coherence_score,
                'num_topics': num_topics,
                'model': lda_model
            }
            
        except Exception as e:
            self.logger.error(f"âŒ LDA ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {'topics': [], 'coherence': 0}
    
    def _analyze_bertopic_topics(self, texts: List[str], case_name: str) -> Dict:
        """BERTopic í† í”½ ë¶„ì„"""
        try:
            if not self.sentence_model:
                self.logger.warning("âš ï¸ SBERT ëª¨ë¸ì´ ì—†ì–´ BERTopic ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return {'topics': [], 'coherence': 0}
            
            if len(texts) < 20:  # ìµœì†Œ ìš”êµ¬ì‚¬í•­ ì¦ê°€
                self.logger.warning("âš ï¸ BERTopic ë¶„ì„ì„ ìœ„í•œ í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                return {'topics': [], 'coherence': 0}
            
            try:
                from bertopic import BERTopic
                from umap import UMAP
                from hdbscan import HDBSCAN
                from sklearn.feature_extraction.text import CountVectorizer
            except ImportError as e:
                self.logger.warning(f"âš ï¸ BERTopic ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì‹¤íŒ¨: {str(e)}")
                return {'topics': [], 'coherence': 0}
            
            # ë™ì  íŒŒë¼ë¯¸í„° ì¡°ì •
            doc_count = len(texts)
            
            try:
                # UMAP ì„¤ì • - ë” ë³´ìˆ˜ì ìœ¼ë¡œ
                umap_model = UMAP(
                    n_neighbors=min(10, max(2, doc_count // 15)),
                    n_components=min(3, max(2, doc_count // 30)),
                    min_dist=0.1,
                    metric='cosine',
                    random_state=42,
                    low_memory=True
                )
                
                # HDBSCAN ì„¤ì • - ë” ë³´ìˆ˜ì ìœ¼ë¡œ
                hdbscan_model = HDBSCAN(
                    min_cluster_size=max(3, min(8, doc_count // 25)),
                    metric='euclidean',
                    cluster_selection_method='eom',
                    prediction_data=False  # ë©”ëª¨ë¦¬ ì ˆì•½
                )
                
                # CountVectorizer ì„¤ì • - ë” ë³´ìˆ˜ì ìœ¼ë¡œ
                vectorizer_model = CountVectorizer(
                    ngram_range=(1, 1),  # unigramë§Œ
                    min_df=max(1, doc_count // 200),
                    max_df=0.95,
                    max_features=min(200, doc_count // 3)
                )
                
                # BERTopic ëª¨ë¸ ìƒì„±
                topic_model = BERTopic(
                    embedding_model=self.sentence_model,
                    umap_model=umap_model,
                    hdbscan_model=hdbscan_model,
                    vectorizer_model=vectorizer_model,
                    language='korean',
                    calculate_probabilities=False,  # ë©”ëª¨ë¦¬ ì ˆì•½
                    verbose=False
                )
                
                # í† í”½ ëª¨ë¸ í•™ìŠµ
                topics, _ = topic_model.fit_transform(texts)
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ BERTopic ëª¨ë¸ ìƒì„±/í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
                return {'topics': [], 'coherence': 0}
            
            # í† í”½ ì •ë³´ ì¶”ì¶œ
            try:
                topic_info = topic_model.get_topic_info()
                topic_results = []
                
                for topic_id in topic_info['Topic'].unique():
                    if topic_id != -1 and len(topic_results) < 5:  # ìµœëŒ€ 5ê°œ í† í”½ë§Œ
                        topic_words = topic_model.get_topic(topic_id)
                        if topic_words and len(topic_words) > 0:
                            # ìƒìœ„ 5ê°œ ë‹¨ì–´ë§Œ
                            top_words = topic_words[:5]
                            topic_results.append({
                                'id': topic_id,
                                'words': top_words,
                                'label': f"í† í”½{topic_id}: {', '.join([word for word, _ in top_words[:3]])}"
                            })
                
                return {
                    'topics': topic_results,
                    'coherence': 0,  # BERTopic ì¼ê´€ì„± ì ìˆ˜ëŠ” ë³„ë„ ê³„ì‚° í•„ìš”
                    'num_topics': len(topic_results),
                    'model': topic_model
                }
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ BERTopic í† í”½ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                return {'topics': [], 'coherence': 0}
            
        except Exception as e:
            self.logger.error(f"âŒ BERTopic ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {'topics': [], 'coherence': 0}
    
    def _compare_topic_methods(self, topic_results: Dict) -> Dict:
        """í† í”½ ëª¨ë¸ë§ ë°©ë²• ê°„ ë¹„êµ"""
        try:
            comparison = {}
            
            for method, result in topic_results.items():
                comparison[method] = {
                    'num_topics': result.get('num_topics', 0),
                    'coherence': result.get('coherence', 0),
                    'top_topics': result.get('topics', [])[:3]
                }
            
            return comparison
            
        except Exception as e:
            self.logger.error(f"âŒ í† í”½ ë°©ë²• ë¹„êµ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def _analyze_topic_evolution(self, df: pd.DataFrame, topic_results: Dict) -> Dict:
        """í† í”½ ì§„í™” ë¶„ì„"""
        try:
            # ê°„ë‹¨í•œ í† í”½ ì§„í™” ë¶„ì„ (ì‹œê°„ë³„ í† í”½ ë¶„í¬ ë³€í™”)
            evolution = {}
            
            # ì‹œê°„ ì»¬ëŸ¼ ì°¾ê¸°
            date_col = self._find_date_column(df)
            if not date_col:
                return {}
            
            # ì›”ë³„ ê·¸ë£¹í™”
            df['month'] = pd.to_datetime(df[date_col]).dt.to_period('M')
            monthly_groups = df.groupby('month')
            
            for month, group in monthly_groups:
                if len(group) > 5:  # ìµœì†Œ ëŒ“ê¸€ ìˆ˜
                    month_texts = self._preprocess_texts_for_topic_modeling(group)
                    if month_texts:
                        evolution[str(month)] = {
                            'comment_count': len(group),
                            'text_count': len(month_texts)
                        }
            
            return evolution
            
        except Exception as e:
            self.logger.error(f"âŒ í† í”½ ì§„í™” ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def _calculate_simple_coherence_score(self, lda_model, tokenized_texts, dictionary) -> float:
        """
        ê°„ë‹¨í•œ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° (ë©€í‹°í”„ë¡œì„¸ì‹± ì—†ì´)
        Args:
            lda_model: LDA ëª¨ë¸
            tokenized_texts: í† í°í™”ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            dictionary: gensim ì‚¬ì „
        Returns:
            ì¼ê´€ì„± ì ìˆ˜ (0-1)
        """
        try:
            if not lda_model or not tokenized_texts or not dictionary:
                return 0.0
            
            # ë§¤ìš° ê°„ë‹¨í•œ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
            # í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ë“¤ì´ ê°™ì€ ë¬¸ì„œì— í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” ë¹ˆë„ ê³„ì‚°
            coherence_scores = []
            
            for topic_id in range(min(lda_model.num_topics, 5)):  # ìµœëŒ€ 5ê°œ í† í”½ë§Œ ì²˜ë¦¬
                try:
                    topic_words = [word for word, _ in lda_model.show_topic(topic_id, topn=3)]  # ë‹¨ì–´ ìˆ˜ ê°ì†Œ
                    
                    if len(topic_words) < 2:
                        continue
                    
                    # í† í”½ ë‹¨ì–´ë“¤ì´ ê°™ì€ ë¬¸ì„œì— í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” ë¹ˆë„ ê³„ì‚°
                    co_occurrence_count = 0
                    total_pairs = 0
                    
                    # ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜ ì œí•œ
                    sample_docs = tokenized_texts[:min(50, len(tokenized_texts))]
                    
                    for doc in sample_docs:
                        if not doc or len(doc) < 2:
                            continue
                            
                        try:
                            doc_set = set(doc)
                            topic_words_in_doc = [word for word in topic_words if word in doc_set]
                            
                            if len(topic_words_in_doc) > 1:
                                # ë¬¸ì„œ ë‚´ì—ì„œ í† í”½ ë‹¨ì–´ë“¤ì˜ ì¡°í•© ìˆ˜
                                pairs_in_doc = len(topic_words_in_doc) * (len(topic_words_in_doc) - 1) / 2
                                co_occurrence_count += pairs_in_doc
                            
                            # ì „ì²´ ê°€ëŠ¥í•œ ì¡°í•© ìˆ˜
                            total_pairs += len(topic_words) * (len(topic_words) - 1) / 2
                        except Exception:
                            continue
                    
                    if total_pairs > 0:
                        topic_coherence = co_occurrence_count / total_pairs
                        coherence_scores.append(topic_coherence)
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í† í”½ {topic_id} ì¼ê´€ì„± ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
                    continue
            
            # í‰ê·  ì¼ê´€ì„± ì ìˆ˜ ë°˜í™˜
            if coherence_scores:
                return float(np.mean(coherence_scores))
            else:
                return 0.0
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê°„ë‹¨í•œ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return 0.0 